
 
1. Introduction

	In this information-technology age, everyday tasks are more and more related to computer.  That ranges from basic jobs such as providing food recipes for housewives to complicated ones such as analyzing laboratory experimental data for scientists and engineers.  This popularity of computer means that the time one has to spend with computer would be a lot more than in the past.

Until now, the computers and computer peripherals in the market have been made according to the same design as the ones invented decades ago when computers are used only in large-scale scientific projects or big corporations.  That means for most people the ergonomic value of these products obviously was not taken into account when designing them.  Fortunately, at the moment, more companies are trying to change the way people work with computer by marketing a number of ergonomic products most notably keyboard, mouse and monitor.  There are ergonomic keyboards, mice and monitors being released all the time.  The reason why the focus is on these products is that they are the parts of computer one interfaces with the most while working with computer.

The subject of whether these ergonomic keyboards, mice, monitors and other products really work attracts a lot of regular computer users.  Thus, studies dedicated to it have been done.  This report is based on one of the studies about an ergonomic keyboard from a manufacturer called Kinesis.

This study looks not only on the effect of the keyboard on the users' body by mean of electromyographic activity but also on the learning rate of the users changing to this new style of keyboard.  This is very useful since slow learning rate would lead to the decrease in effectiveness of work.

Introduced in 1868 by Christopher Sholes, computer keyboard is still the primary data entry mode for most computer users.  With the increase of computer, hence keyboard, usage at the moment, these problems of the keyboard users known as operator stress problems have developed.  This is a kind of cumulative trauma disorders which is mainly caused by working excessively or repetitively with the same thing, keyboard, in this case, in the same position for a long period of time.  This kind of disorder is considered to be the most expensive and severe one occurring in office environment.

This leads to an amount of alternative designs introduced in the market with the main intention of reducing muscular stress required for typing.  The reason why these designs have not yet replaced the old one is because of the familiarity of the users to the old design.  This means an amount of retraining time is required to familiarize the users to a new design of keyboard and thus the one requiring less time is likely to be the choice.

This study main objectives are to measure and analyze initial learning rate and electromyographic activity, explained later, while using an alternative design of keyboard, the Kinesis Ergonomic Computer Keyboard (figure1.)  These data are then used to compare to the standard computer keyboard, the old design, to see if it is worth the time and money spent on the new product.

The electromyographic signals used to examine the muscle activities in this study are signals generated by muscles.  These signals can sometimes be used to control artificial body limbs especially ones requiring sensitive or complicated degree of control such as rotary or grasping motion.  Systems that use such signals are called myoelectric systems.

The Kinesis keyboard utilizes the same QWERTY layout as the standard design so that users do not have to relearn typing all over again.  The key ergonomic features of this keyboard are:
 The distance between centers of the halves of the Kinesis keyboard is approximately 27 cm, reducing the angle of adduction of the wrists to near zero for most adults.
 The keypads slope downward from inside to outside edge, and are concave to better fit the natural shape of the operator's hands.  The keys form straight columns and slightly curved rows.
 The keyboard features a built-in forearm-wrist support extending approximately 14 cm from the home row to the edge.
 The keyboard features separate thumb-operated keypads to redistribute the workload from the little fingers to the thumbs.  These keypads consists of the enter, space, backspace, delete and combination (ctrl and alt) keys.
 Detachable numeric/cursor pad.
 Integral palm supports.
 Shorter reach for function keys.

 
Figure 1.  The Kinesis Ergonomic Computer Keyboard.

 2. Details

2.1 Materials and methods

	There were 6 female professional typists participants of age 29 to 52 and typing experience of 10 to 32 years involved in this experiment.  Typing speed in words per minute, typing accuracy in percentage of characters typed correctl 



 
Communications. I could barely spell the word, much less comprehend its meaning. Yet when
Mrs. Rubin made the announcement about the new club she was starting at the junior high school, it triggered something in
my mind. 

Two weeks later, during the last month of my eighth grade year, I figured it out. I was rummaging through the basement, and
I ran across the little blue box that my dad had brought home from work a year earlier. Could this be a modem? 

I asked Mrs. Rubin about it the next day at school, and when she verified my expectations, I became the first member of
Teleport 2000, the only organization in the city dedicated to introducing students to the information highway. 

This was when 2400-baud was considered state-of-the-art, and telecommunications was still distant from everyday life. But
as I incessantly logged onto Cleveland Freenet that summer, sending e-mail and posting usenet news messages until my
fingers bled, I began to notice the little things. Electronic mail addresses started popping up on business cards. Those
otherwise-incomprehensible computer magazines that my dad brought home from work ran monthly stories on
communications-program this, and Internet-system that. Cleveland Freenet's Freeport software began appearing on systems
all over the world, in places as far away as Finland and Germany - with free telnet access! 

I didn't live life as a normal twelve-year-old kid that summer. I sat in front of the monitor twenty-four hours a day, eating my meals from a plate set next to the
keyboard, stopping only to sleep. When I went back to school in the fall, I was elected the first president of Teleport 2000, partially because I was the only student
in-the school with a freenet account, but mostly because my enthusiasm for this new, exciting world was contagious. 

Today, as the business world is becoming more aware of the advantages of telecommunications, and the younger generation is becoming more aware of the
opportunities, it is successfully being integrated into all aspects of our society. Companies are organizing Local Area Networks and tapping into information
resources through internal networking and file sharing, and children of all ages are entertained by the GUI-based commercial systems and amazed by the worldwide
system of gopher and search services. As a result, a million more people join the 'net every month, according to a 1994 article by Vic Sussman in U.S. News
& World Report. 

They say that the worldwide community used to double its knowledge every century. Right now, that rate has been reduced to seven years, and is constantly
decreasing. I've learned more since I started traveling the information highway than I could have possibly imagined. Through File Transfer Protocol sites, I can
download anything from virus-detection utilities to song lyrics and guitar tabs. I receive press releases, proclamations and international news from the White House
via a mailing list. I even e-mailed President Clinton recently and received a response the next day. And it was just a few months ago that I hung up my
2400-baud modem for a replacement six times as fast. 

The essence of this international system of systems was neatly summed up by David S. Jackson and Suneel Ratan in a recent Time article: "The magic of the Net is
that it thrusts people together in a strange new world, one in which they get to rub virtual shoulders with characters they might otherwise never meet." 

To me, this electronic "Cyberspace" was like kindergarten all over again. It was not only an introduction to a whole new world of exciting opportunities, but it
helped me take a step further into maturity. Communicating with others on this alternate plane of reality was so different, yet so similar, to the world I had already
experienced. The Internet is a place where the only way you can view people is by how they choose to display themselves. Because you can't see other users,
you can't make any prejudgments based upon race, sex, or physical handicap. As stated by John R. Levine and Carol Baroudi in The Internet for Dummies,
'Who you are on the Internet depends solely on how you present yourself through your keyboard." 

The reason for this is simple. The people who created this form of communication weren't interested in that. They didn't care about political or ethnic boundaries;
they only cared about the abstract. As a result, the parallel world they conceived contained a true form of equality. "One computer is no better than any other, and
no person is better than any other," wrote Levine and Baroudi, and the only way this right can be taken away from you is if you choose to remove it yourself. My
realization of this concept taught me a lot about the faults of the real world, and why so many people feel the need to defect to Cyberspace so frequently. 

I believe in the future - not the extreme 1984; 2001: A Space Odyssey future, but the inevitable progression from today into tomorrow. The people of tomorrow
will not be puzzled by the word "Internet" or the mechanics behind networking - these will be basic survival skills in society. The future will see an
electronically-linked global community, in which everyone is a citizen. The constant thickening of the worldwide web of networks excites me, because it
proves that the world is not as big as one may think. You really can reach out to anyone you want in a matter of milliseconds. 

The other day, I was helping a ten-year-old girl find an e-mail "key-pal" from Australia. I think I see a lot of me, the curious eighth-grader, in her. Perhaps I see a lot
of the future, too.  

 
	You have probably heard of the Internet, but you weren't really sure if it was for you.   You thought about it, but after all it costs so much and things like pornography and improper language are used everywhere, right?  Wrong!  Perhaps, I can convince you that America Online will be worth your time and money. 
	One of the main reasons that people don't go online is that they think that it costs too much.  America Online or AOL doesn't really cost all that much.  When you sign on you get from 10 to 50 hours free, depending on the software that you download.  Once you run out of free hours you may choose to stay online with a monthly fee.  This monthly fee can be either $9.95 or $19.95 depending on how many hours you plan on using. 	If  you are concerned that your children will visit web pages you prefer that they don't, then you can put parental guards on that don't allow them to visit those web pages.  If you aren't familiar with web pages, they are basically ads that you look at containing information about the company, person, or product.  Also you can sign your child on as a child or teen which keeps them out of restricted areas. 	Perhaps your main concern is people finding out things that you don't want them to.  They only know as much as you tell them.  If they ask for your password, credit card number, or any other personal info, you don't have to tell them that information.   When you first sign on AOL staff will  ask for things like name, age, address, phone number, and your credit card or checking account number.  These things remain confidential and are used only for billing purposes.
	If anyone ask for personal information you can easily report them to AOL.  When someone is reported they are either warned or kicked off the Internet.  You can also report people that swear or use any kind of offensive words. 	Many of the chat rooms are guarded by "online hosts" or people that belong to AOL.  These "guards" make sure nothing bad happens in chat rooms.  You can be sure that there are AOL staff in the romance rooms, especially, because that is where the most foul and vulgar language takes place.  If you are too young to be in the room, they will tell you to leave and go to a room where people your age belong.
	The world "online" also offers thousands of Reference sources like Groliers Multimedia Encyclopedia and over 100 magazines. These Magazines alone are of great value to anyone who enjoys reading magazines.     These References will tell you almost anything, but if you wanted to know about something that was not in these sources, then you can leave the New York Public Library's librarian  a message.  This person will respond within the week to your question.
	Finally, with your America Online subscription you get unlimited E-Mail.  What is E-Mail you ask?   Well E-Mail stands for electronic mail.  It is a way to send letters to anyone in the world that is hooked up to The Internet or other online services.  This mail is received almost instantly, within a few seconds. This way you could send letters to a pen pal in Egypt. Instead of waiting up to a month or more, he  will receive it the same day. 
	Having America Online opens you up to a whole new world of information and people.  America Online provides an inexpensive yet secure place for work, education, and recreation.  A family has so much to gain and little to lose by signing on today.


 
John Hassler	
Professor C. Mason
Computer Information systems 204
September 13, 1996

Application Software
	Computer systems contain both hard and software.  Hardware is any tangible item in a computer system, like the system unit, keyboard, or printer.  Software, or a computer program, is the set of instruction that direct the computer to perform a task.  Software falls into one of two categories: system software and application software.  System software controls the operation of the computer hardware; whereas, application software enables a user to perform tasks.  Three major types of application software on the market today for personal computers are word processors, electronic spreadsheets, and database management systems (Little and Benson 10-42).
	A word processing program allows a user to efficiently and economically create professional looking documents such as memoranda, letters, reports, and resumes.  With a word processor, one can easily revise a document.  To improve the accuracy of one's writing, word processors can check the spelling and the grammar in a document.  They also provide a thesaurus to enable a user to add variety and precision to his or her writing.  Many word processing programs also provide desktop publishing features to create brochures, advertisements, and newsletters.
	An electronic spreadsheet enables a user to organize data in a fashion similar to a paper spreadsheet.  The difference is the user does not have to perform calculations manually; electronic spreadsheets can be instructed to perform any computation desired.  The contents of an electronic spreadsheet can be easily modified by the user.  Once the data is modified, all calculations in the spreadsheet are recomputed automatically.  Many electronic spreadsheet packages also enable a user to graph the data in his or her spreadsheet (Wakefield 98-110).
	A database management system (DBMS) is a software program that allows a user to efficiently store a large amount of data in a centralized location.  Data is one of the most valuable resources to any organization.  For this reason, user desire data be organized and readily accessible in a variety of formats.  With aDBMS, a user can then easily store data, retrieve data, modify data, analyze data, and create a variety of reports from the data(Aldrin 25-37).
	Many organizations today have all three of these types of application software packages installed on their personal computers.  Word processors, electronic spreadsheets, and database management systems make users' tasks more efficient.  When users are more efficient, the company as a whole operates more economically and efficiently.


orks Cited
Aldrin, James F. "A Discussion of Database Management Systems." Database Monthly May 	1995: 25-37.
Little, Karen A. And Jeffrey W. Benson. Word Processors. Boston: Boyd Publishing Company, 
	1995.
Wakefield, Sheila A. "What Can An Electronic Spreadsheet Do For You," PC Analyzer Apr.
	1995: 98-110.


 
	           Argumentative Essay 			


	In the summer of 1996 Gwen Jacobs enjoyed a topless summer stroll during which she was seen by a local O.P.P officer, was apprehended and subsequently charged with indecent exposure.  Gwen Jacobs pleaded not guilty in court and won the right to go topless in Ontario.  This incident brought up an excellent question: should women be allowed to go topless on public beaches and in other public areas?  The answer is strictly no, women should not be allowed to go topless anywhere outside of their own home.  
	One of the many reasons why I believe that women should not be allowed to go topless is with respect to the safety of women.  Men and boys have, in recent years, been using short, tight, skirts and shirts as an excuse for rape or date rape.   Men have said that the girl was wearing a tight shirt and short skirt and it was obvious that she was easy and wanted the attention.  This statement leads me to my next point.
	The average human being upon first contact with a stranger bases his initial impression of that person solely on the person's appearance.  This is only natural as the only thing that we know about this stranger is what we see of them the first time we meet.  We all are aware of the sayings "Preppy","Jockish","Skater","Sluty" etc.  This final saying, "Sluty" is interpreted by 90 percent of North Americans as a tight skirt and tight tank top which happens to be the usual ensemble of a prostitute.  This first impression of a girl in nothing but a skirt and a bare chest will no doubt elevate to the new version of a "Slut" and a girl that wants it.
	My second point is, what kind of questions will a mother be asked by her son when he sees a half nude woman walking down the street.  The first question that this child will ask is why do these women have no shirt on and you do?  Your reply will be well ahhh go talk to your father.  This dilemma will no doubt be brought about as these and other questions about the sexual nature of the body will be put forth by young children.  Questions that you as a parent do not feel should be answered truthfully to such a young child.
	My third point begins thousands of years ago when man first walked on the earth.  When man first walked he hunted and his wife(clothless) cleaned the game and took care of the young.  As centuries have progressed women have stepped forth into a new era of equal rights.  We've seen the first women doctors, astronauts, business owners and many other firsts in numerous professions.  Women have made giant leaps when it comes to respect from men in their professional field.  This respect which women have been fighting for over the past century, is on the verge of collapse.  Women seem to be taking this new law allowing them to go topless to an extreme.  Walking their dogs, walking on the beach and strolling through public places with no tops on.  This display of nudity, in the average person's eyes, whether they admit to it or not,  will cause men to look down again on women.  If, for example, the first woman astronaut (Sally Ride) were to start going topless in public places it would be plastered on the front page of every newspaper.  This in turn would lead to her fellow colleagues looking down on her.  This would be a giant step backwards in respect to equal rights for women.
	Following the changes to this law allowing women to go topless our cities will slowly begin to diverge into places that encourage nudity and places that do not encourage nudity.  Our economy will begin to collapse, as store owners appalled by this nudity will be forced to close their stores and move, if this nudity is surrounding them.  This also applies to stores that want to have workers that want to go topless, they will be forced to relocate to places of nudity.  As this begins to happen slowly our cities will become two sided and our economy's stability will collapse beneath our feet.  An excellent example of this situation is taking place in Quebec. A law in Quebec states that a women may work in nothing less than lingerie.   So a Quebec barber shop run by a well endowed women decided to charge an extra ten dollars per haircut and she'd remove her shirt so they could watch her cut their hair in just a bra.  She also charged an extra fifteen to remove her bottoms so she had only her underwear on.  This new business skyrocketed and now there is currently 15 of these hair dressers presently in Quebec.  The neighborhoods surrounding these barbershops are appalled by what is going on and many people have relocated there families away from this nudity.
	In conclusion to the question: should women be allowed to go topless in public places?   It has been clearly shown that women should not be allowed to go topless anywhere outside of their own home. 


 
 
ABSTRACT

Current neural network technology is the most progressive of the artificial intelligence 
systems today.  Applications of neural networks have made the transition from laboratory 
curiosities to large, successful commercial applications.  To enhance the security of automated 
financial transactions, current technologies in both speech recognition and handwriting 
recognition are likely ready for mass integration into financial institutions.

RESEARCH PROJECT
TABLE OF CONTENTS
Introduction	1
Purpose	1
Source of Information 	1
Authorization	1
Overview	2
The First Steps	3
Computer-Synthesized Senses	4
Visual Recognition	4
Current Research	5
Computer-Aided Voice Recognition	6
 Current Applications	7
Optical Character Recognition	8
Conclusion	9
Recommendations	10
Bibiography	11


INTRODUCTION

 Purpose   

	The purpose of this study is to determine additional areas where artificial intelligence 
	technology may be applied for positive identifications of individuals during financial 
	transactions, such as automated banking transactions, telephone transactions , and home 
	banking activities.  This study focuses on academic research in neural network technology . 
	This study was funded by the Banking Commission in its effort to deter fraud.
Overview

	Recently, the thrust of studies into practical applications for artificial intelligence 
	have focused on exploiting the expectations of both expert systems and neural network 
	computers.  In the artificial intelligence community, the proponents of expert systems 
	have approached the challenge of simulating intelligence differently than their counterpart 
	proponents of neural networks. Expert systems contain the coded knowledge of a human expert 
	in a field; this knowledge takes the form of "if-then" rules.  The problem with this approach 
	is that people don't always know why they do what they do. And even when they can express this 
	knowledge, it is not easily translated into usable computer code. Also, expert systems are 
	usually bound by a rigid set of inflexible rules which do not change with experience gained 
	by trail and error. In contrast, neural networks are designed around the structure of a 
	biological model of the brain.  Neural networks are composed of simple components called 
	"neurons" each having simple tasks, and simultaneously communicating with each other by 
	complex interconnections.  As Herb Brody states, "Neural networks do not require an explicit 
	set of rules. The network - rather like a child - makes up its own rules that match the 
	data it receives to the result it's told is correct" (42).  Impossible to achieve in expert 
	systems, this ability to learn by example is the characteristic of neural networks that makes
	them best suited to simulate human behavior. Computer scientists have exploited this system 
	characteristic to achieve breakthroughs in computer vision, speech recognition, and optical
	character recognition.  Figure 1 illustrates the knowledge structures of neural networks 
	as compared to expert systems and standard computer programs. Neural networks restructure 
	their knowledge base at each step in the learning process.
	This paper focuses on neural network technologies which have the potential to increase security 
	for financial transactions.  Much of the technology is currently in the research phase and has 
	yet to produce a commercially available product, such as visual recognition applications.  
	Other applications are a multimillion dollar industry and the products are well known, like 
	Sprint Telephone's voice activated telephone calling system.  In the Sprint system the neural 
	network positively recognizes the caller's voice, thereby authorizing activation of his 
	calling account.


The First Steps

	The study of the brain was once limited to the study of living tissue.  Any attempts at an 
	electronic simulation were brushed aside by the neurobiologist community as abstract conceptions 
	that bore little relationship to reality.  This was partially due to the over-excitement in 
	the 1950's and 1960's for networks that could recognize some patterns, but were limited in 
	their learning abilities because of hardware limitations. In the 1990's computer simulations 
	of brain functions are gaining respect as the simulations increase their abilities to predict 
	the behavior of the nervous system. This respect is illustrated by the fact that many 
	neurobiologists are increasingly moving toward neural network type simulations.  One such 
	neurobiologist, Sejnowski, introduced a three-layer net which has made some excellent predictions 
	about how biological systems behave.  Figure 2 illustrates this network consisting of three 
	layers, in which a middle layer of units connects the input and output layers. When the network 
	is given an input, it sends signals through the middle layer which checks for correct output.  
	An algorithm used in the middle layer reduces errors by strengthening or weakening connections 
	in the network.  This system, in which the system learns to adapt to the changing conditions, 
	is called back-propagation. The value of Sejnowski's network is illustrated by an experiment 
	by Richard Andersen at the Massachusetts Institute of Technology.  Andersen's team spent years 
	researching the neurons monkeys use to locate an object in space (Dreyfus and Dreyfus 42-61).  
	Anderson decided to use a neural network to replicate the findings from their research.  They 
	"trained" the neural network to locate objects by retina and eye position, then observed 
	the middle layer to see how it responded to the input.  The result was nearly identical to what 
	they found in their experiments with monkeys. 

Computer-Synthesized Senses
 Visual Recognition
	The ability of a computer to distinguish one customer from another is not yet a reality.  But, recent breakthroughs in neural network visual technology are bringing us closer to the time when computers will positively identify a person.
 Current Research

	Studying the retina of the eye is the focus of research by two professors at the California 
	Institute of Technology, Misha A. Mahowald and Carver Mead.  Their objective is to electronically 
	mimic the function of the retina of the human eye. Previous research in this field consisted 
	of processing the absolute value of the illumination at each point on an object, and required 
	a very powerful computer.(Thompson 249-250).  The analysis required measurements be taken over 
	a massive number of sample locations on the object, and so, it required the computing power of a 
	massive digital computer to analyze the data.
	The professors believe that to replicate the function of the human retina they can use a neural 
	network modeled with a similar biological structure of the eye, rather than simply using massive 
	computer power.  Their chip utilizes an analog computer which is less powerful than the previous 
	digital computers.  They compensated for the reduced computing power by employing a far more 
	sophisticated neural network to interpret the signals from the electronic eye.  They modeled the 
	network in their silicon chip based on the top three layers of the retina which are the best 
	understood portions of the eye.(250)  These are the photoreceptors, horizontal cells, and bipolar cells.
	The electronic photoreceptors, which make up the first layer, are like the rod and cone cells in the eye.  
	Their job is to accept incoming light and transform it into electrical signals.  In the second 
	layer, horizontal cells use a neural network technique by interconnecting the horizontal cells 
	and the bipolar cells of the third layer.  The connected cells then evaluate the estimated 
	reliability of the other cells and give a weighted average of the potentials of the cells 
	around it.  Nearby cells are given the most weight and far cells less weight.(251)  
	This technique is very important to this process because of the dynamic nature of image 
	processing. If the image is accepted without testing its probable accuracy, the likelihood 
	of image distortion would increase as the image changed.
	The silicon chip that the two professors developed contains about 2,500 pixels- photoreceptors 
	and their associated image-processing circuitry.  The chip has circuitry that allows a professor 
	to focus on each pixel individually or to observe the whole scene on a monitor.  The professors 
	stated in their paper, "The behavior of the adaptive retina is remarkably similar to that of 
	biological systems" (qtd in Thompon 251).

	The retina was first tested by changing the light intensity of just one single pixel while the 
	intensity of the surrounding cells was kept at a constant level.  The design of the neural network 
	caused the response of the surrounding pixels to react in the same manner as in biological retinas.      
	They state that, "In digital systems, data and computational operations must be converted into 
	binary code, a process that requires about 10,000  digital voltage changes per operation. 
	Analog devices carry out the same operation in one step and so decrease the power  consumption 
	of silicon circuits by a factor of about 10,000" (qtd in Thompson 251).  
	Besides validating their neural network, the accuracy of this silicon chip displays the usefulness 
	of analog computing despite the assumption that only digital computing can provide the accuracy 
	necessary for the processing of information.
	As close as these systems come to imitating their biological counterparts, they still have a long 
	way to go.  For a computer to identify more complex shapes, e. g., a person's face, the professors 
	estimate the requirement would be at least 100 times more pixels as well as additional circuits 
	that mimic the movement-sensitive and edge-enhancing functions of the eye.  They feel it is possible 
	to achieve this number of pixels in the near future.  When it does arrive, the new technology will 
	likely be capable of recognizing human faces.
	Visual recognition would have an undeniable effect on reducing crime in automated financial transactions.  
	Future technology breakthroughs will bring visual recognition closer to the recognition of individuals, 
	thereby enhancing the security of automated financial transactions.

 Computer-Aided Voice Recognition

	Voice recognition is another area that has been the subject of neural network research.  
	Researchers have long been interested in developing an accurate computer-based system capable 
	of understanding human speech as well as accurately identifying one speaker from another.


 Current Research

	Ben Yuhas, a computer engineer at John Hopkins University, has developed a promising system for 
	understanding speech and identifying voices that utilizes the power of neural networks.  Previous attempts 
	at this task have yielded systems that are capable of recognizing up to 10,000 words, but only when each 
	word is spoken slowly in an otherwise silent setting.  This type of system is easily confused by back 
	ground noise (Moyne 100).
	Ben Yuhas' theory is based on the notion that understanding human speech is aided, to some small degree, 
	by reading lips while trying to listen.  The emphasis on lip reading is thought to increase as the 
	surrounding noise levels increase.  This theory has been applied to speech recognition by adding a 
	system that allows the computer to view the speaker's lips through a video analysis system while 
	hearing the speech.
	The computer, through the neural network, can learn from its mistakes through a training session. Looking 
	at silent video stills of people saying each individual vowel, the network developed a series of 
	images of the different mouth, lip, teeth, and tongue positions.  It then compared the video images 
	with the possible sound frequencies and guessed which combination was best.  
	Yuhas then combined the video recognition with the speech recognition systems and input a video frame 
	along with speech that had background noise.  The system then estimated the possible sound frequencies 
	from the video and combined the estimates with the actual sound signals. After about 500 trial runs the 
	system was as proficient as a human looking at the same video sequences.
	This combination of speech recognition and video imaging substantially increases the security factor by 
	not only recognizing a large vocabulary, but also by identifying the individual customer using the system.

 Current Applications

	Laboratory advances like Ben Yuhas' have already created a steadily increasing market in speech recognition.  
	Speech recognition products are expected to break the billion-dollar sales mark this year for the first time.  
	Only three years ago, speech recognition products sold less than $200 million (Shaffer, 238).
	Systems currently on the market include voice-activated dialing for cellular phones, made secure by their 
	recognition and authorization of a single approved caller.  International telephone companies such as Sprint 
	are using similar voice recognition systems.  Integrated Speech Solution in Massachusetts is investigating 
	speech applications which can take orders for mutual funds prospectuses and account activities (239).

 Optical Character Recognition

	Another potential area for  transaction security is in the identification of handwriting by optical 
	character recognition systems (OCR).  In conventional OCR systems the program matches each letter in a 
	scanned document with a pre-arranged template stored in memory.  Most OCR systems are designed specifically 
	for reading forms which are produced for that purpose.  Other systems can achieve good results with 
	machine printed text in almost all font styles.  However, none of the systems is capable of recognizing 
	handwritten characters.  This is because every person writes differently.  
	Nestor, a company based in Providence, Rhode Island has developed handwriting recognition products based 
	on developments in neural network computers.  Their system, NestorReader, recognizes handwritten characters 
	by extracting data sets, or feature vectors, from each character.  The system processes the input 
	representations using a collection of three by three pixel edge templates (Pennisi, 23).  The system then 
	lays a grid over the pixel array and pieces it together to form a letter.   Then the network discovers 
	which letter the feature vector most closely matched.  The system can learn through trial and error, 
	and it has an accuracy of about 80 percent.  Eventually this system will be able to evaluate all symbols 
	with equal accuracy.
	It is possible to implement new neural-network based OCR systems into standard large optical systems.  
	Those older systems, used for automated processing of forms and documents, are limited to reading typed 
	block letters. When added to these systems, neural networks improve accuracy of reading not only typed 
	letters but also handwritten characters.  Along with automated form processing, neural networks will 
	analyze signatures for possible forgeries.


Conclusion

	Neural networks are still considered emerging technology and have a long way to go toward achieving their 
	goals.  This is certainly true for financial transaction security. But with the current capabilities, 
	neural networks can certainly assist humans in complex tasks where large amounts of data need to be analyzed. 
	For visual recognition of individual customers, neural networks are still in the simple pattern matching 
	stages and will need more development before commercially acceptable products are available.  Speech 
	recognition, on the other hand, is already a huge industry with customers ranging from individual computer 
	users to international telephone companies.  For security, voice recognition could be an added link to the 
	chain of pre-established systems.  For example, automated account inquiry, by telephone, is a popular method 
	for customers to determine the status of existing accounts.  With voice identification of customers, an 
	option could be added for a customer to request account transactions and payments to other institutions.
	For credit card fraud detection, banks have relied on computers to identify suspicious transactions.  
	In fraud detection, these programs look for sudden changes in spending patterns such as large cash withdrawals 
	or erratic spending.  The drawback to this approach is that there are more accounts flagged for possible 
	fraud than there are investigators.  The number of flags could be dramatically reduced with optical character 
	recognition to help focus investigative efforts.
	It is expected that the upcoming neural network chips and add-on boards from Intel will add blinding speed 
	to the current network software.  These systems will even further reduce losses due to fraud by enabling 
	more data to be processed more quickly and with greater accuracy.
Recommendations
	Breakthroughs in neural network technology have already created many new applications in financial transaction 
	security.  Currently, neural network applications focus on processing data such as loan applications, and 
	flagging possible loan risks.  As computer  hardware speed increases and as neural networks get smarter, 
	"real-time" neural network applications should become a reality.  "Real-time" processing means the network 
	processes the transactions as they occur.  
	In the mean time,
1.	Watch for advances in visual recognition hardware / neural networks. When available, commercially produced 
	visual recognition systems will greatly enhance the security of automated financial transactions. 

2.	Computer aided voice recognition is already a reality. This technology should be implemented in automated 
	telephone account inquiries. The feasibility of adding phone transactions should also be considered.  
	Cooperation among financial institutions could result in secure transfers of funds between banks when 
	ordered by the customers over the telephone.

3.	Handwriting recognition by OCR systems should be combined with existing check processing systems.  
	These systems can reject checks that are possible forgeries.  Investigators could follow-up on the 
	OCR rejection by making appropriate inquiries with the check writer.
BIBLIOGRAPHY

Winston, Patrick.  Artificial Intelligence.  Menlo Park: Addison-Wesley Publishing, 1988.

Welstead, Stephen. Neural Network and Fuzzy Logic in C/C++. New York: Welstead, 1994.

Brody, Herb. "Computers That Learn by Doing." Technology Review August 1990: 42-49.

Thompson, William. "Overturning the Category Bucket." BYTE  January 1991: 249-50+.

Hinton, Geoffrey. "How Neural Networks Learn from Experience." Scientific American September 1992: 145-151.

Dreyfus, Hubert., and Stuart E. Dreyfus. "Why Computers May Never Think Like People." Technology Review  January 1986: 42-61.

Shaffer, Richard. "Computers with Ears." FORBES September 1994: 238-239.

 


 
Artificial Intellegence

Identification And Description Of The Issue

Over the years people have been wanting robots to become more Intelligent. In the past 50 years since computers have been around, the computer world has grown like you wouldn't believe.  Robots have now been given jobs that were 15 years ago no considered to be a robots job.  Robots are now part of the huge American government Agency the FBI.  They are used to disarm bombs and remove dangerous products from a site without putting human life in danger.
You probably don't think that when you are in a carwash that a robotic machine is cleaning your car.  The truth is that they are.  The robot is uses senses to tell the main computer what temperature the water should be and what style of wash the car is getting e.g. Supreme or Normal wash.
Computer robots are being made, that learn from their mistakes.  Computers are now creating their own programs. In the past there used to be some problems,   now they are pretty much full proof.
The Television and Film business has to keep up with the demands from the critics sitting back at home, they try and think of new ideas and ways in which to entertain the audiences.  They have found that robotics interests people.  With that have made many movies about robotics  (e.g.  Terminator, Star Wars, Jurassic Park ).
Movie characters like the terminator would walk, talk and do actions by its self mimicking a human through the use of Artificial Intelligence.
Movies and Television robots don't have Artificial Intelligence ( AI ) but are made to look like they do.  This gives us the viewers a reality of robotics with AI.


Understanding Of The IT Background Of The Issue

Artificial Intelligence means " Behavior performed by a machine that would require some degree of intelligence if carried out by a human ".
The Carwash machine has some intelligence which enables it to tell the precise temperature of the water it is spraying onto your car.  If the water is to hot it could damage the  paint work or even make the rubber seals on the car looser.  The definition above shows that AI is present in everyday life surrounding humans where ever they go.
Alan Turing Invented a way in which to test AI.  This test is called the Turing Test.  A computer asks a human various questions.  Those conducting the test have to decide whether the human or the computer is asking the questions.



Analysis Of The Impact Of The Issue

With the increasing amount of robots with AI in the work place and in everyday life,  it is making human jobs insecure for now and in the future.  If we take a look at all the major car factories 70 years ago they were all hand crafted and machinery was used very little.  Today we see companies like  TOYOTA who produce mass amounts of cars with robots as the workers.  This shows that human workmanship is required less and less needed.
This is bad for the workers because they will then have no jobs and will be on the unemployment benefit or trying to find a new job.
The advantage of robots is that they don't need a coffee break or need to have time of work.  The company owns the machinery and therefore they have control over the robot.


Solutions To Problems Arising From The Issue

Some problems arising from the issue would include job loss, due to robots taking the place of  humans in the work place.  This could be resolved by educating the workers to do other necessary jobs in the production line.  Many of the workers will still keep their other jobs that machines can't do.  
If robots became to intelligent this could be a huge disaster for human kind.  We might end up being second best to robots.  They would have the power to do anything and could eliminate humans from the planet especially if they are able to programme themselves without human help.  I think the chance of this happening is slim but it is a possibility.   



 
Battle of the Bytes
Macintosh vs. Windows 95

	It used to be that the choice between a Mac and a PC was pretty clear. If 

you wanted to go for the more expensive, easier to use, and better graphics and 

sound, you went to buy a Macintosh, for the cheaper price, it was the PC. Now it 

is a much different show. With the release of Windows 95 and the dynamics of 

the hardware market have changed the equation.

	On the other hand, Apple has made great price reductions on many of 

their computers last October. You can now buy a reasonably equipped Power 

Macintosh at about the same price as a PC that has about the same things. This 

makes the competition much harder. 

	Windows 3.x have been great improvements over the earlier versions of 

Windows, and of course over DOS, but it still didn't compete against the ease of 

use on a Mac. The Windows 95 interface is much better than Windows 3.x. It 

borrows some from the Macintosh interface and has improved on it.

	Some improvements are the ability to work with folder icons that represent 

directories and subdirectories in DOS. Windows 95, unlike the Mac, logically 

groups data and resources. A Taskbar menu lets you call up and switch between 

any software application at any time. Thus feature is better than the Mac's 

because its use is more obvious. It clearly shows what is running and allows you 

to switch programs with a single click of the mouse. Control panels have been 

added so you can configure your hardware. There is easy access to frequently 

used files. You can make very long file names on Windows 95 instead of short 

and strange names that leave you wondering about, such as on Windows 3.x I 

could not name a folder This is stuff for school it must be a lot shorter. The Help 

system helps you implement its suggestions. A multilevel Undo command for all 

file operations safeguards your work, something Macintosh does not have. 

Something that Windows 95 has, similar to the Macintosh Alias function, is 

shortcut icons. It calls up a program very easily, instead of searching through 

your hard drive. The Windows 95 shortcuts go beyond the Mac's, they can refer 

to data inside documents as well as to files and folders, and can also call up 

information on a local area network server or Internet site. Windows 95's plug 

and play system allows the operating system to read what's on your machine 

and automatically configure your new software that you need to install, however, 

this only works if the added hardware is designed to support it, and it will for a 

majority of hardware.

	All these things are major improvements, but hardware and CONFIG.SYS 

settings left over from earlier programs can conflict with the new system, causing 

your hard drive to crash. This is something all users of Windows 95 will dread.

	Even though Microsoft has made many wonderful changes to Windows, 

Apple is working on developing a new operation system, called Copland. It may

beat many of the Windows 95 improvements.  Apple is still deciding on what new 

things to add when the system will start shipping later in the year. Some new 

things may be a customizable user interface and features such as drawers, 

built-in indexing and automatically updated search templates to help users 

manger their hard drives much more efficiently. The biggest improvement is to 

be able to network systems from multiple vendors running multiple operating 

systems. Like Windows 95, Copland will also have a single in-box for fax, e-mail, 

and other communications. The disadvantage of Copland is it can only be used 

on Power Macintoshes.

	I would personally go for a PC with Windows 95. I choose it because of 

the many programs that can be used on PC's. Whenever I walk into a computer 

store, such as Electronics Boutique, half of the store is taken up by programs 

that can be used on an IBM compatible PC. There is only one little shelf for 

things that run on Macs. It seems that the more people use PC's. I have met very 

few people with a Macintosh. I can bring many things from my computers to 

theirs and the other way around without worrying, "What if I need to find this for 

a Mac?"

	Schools should use Windows95 PC's because of the many more 

educational programs available for PC's. Since of the making of Windows 95 

many companies now make programs for the PC. It may be a long time, if ever, 

that they will decide to make it for a Mac. Plus since of the many people with IBM 

PC's at home, people can bring their work to and from school. If everyone had 

the same kind of computer on a network, students could go into the computers at 

schools all over the world to use programs there. 

	So since now that the quality of  computers are equal it is very hard to 

make your decision. For those that are not computer literate, the best thing to do 

is to go for the Mac because of the easiness involved in using one. This means 

you get less choice of programs in a store, and if you go online, many people will 

be using something different from you so you have no idea what they are talking 

about. If you know how a computer is basically used, a Windows 95 PC will be 

no problem. It doesn't take that long to learn. You will have a bigger choice of 

programs and may be able to do more things with other people that have a 

computer. It comes down to this choice. Most of the choosing will go to schools 

because of the many using Macintosh computers, which most of Apple's money 

comes from. It is only recently companies that made software for PC's that got 

interested in making programs for educational purposes. 

	So if you are deciding a computer. I leave you to decide this. Windows 95 

or Macintosh, the choice is yours.

	
	I feel that this is the best journal entry I have ever written. It informs the 

reader a great deal about the subject and it helps you make a decision that is 

very important if you decide to buy a computer for work or home use. It is very 

helpful because it can educate people in the world that are not computer literate 

in a world that is being taken over by computers. Things such as the internet are 

used by many people, and it would certainly help if you needed to know what kind 

to buy so your would be compatible with someone else's. This entry tells that I am 

one that is around computers a lot and have an interest in them. 




 
Buoyant Force

	The purpose of this lab is to calculate bouyant forces of objects submerged in water.
	The first step in the lab was to measure the mass of a metal cylinder, which was found to be 100g, and then to calculated it's weight, which was .98 newtons.  Then next step was to measure the apparent weight of the cylinder when it is completely submerged in a bath of water using the formula Wa=ma*g , this was found to be 88.5grams.  Knowing these two numbers, the bouyant force that the water places on the object can be calculated using the formula  Fb=W-Wa ,   Wa=.8673n  W=.98n  Fb=.1127n
	Part 2 of this lab consisted of weighing an empty cup, which was 44grams.    And then filling another cup up to a certain point the if any more water was added, it would spill out of a little opening in the cup, the water spilled out could be caught in the first cup.  This is done so that the water spilled out can be weighed and compared to a calculated weight of which the water should be.  After filling the cup, the cylinder was put into the cup , allowing the water to spill out and be caught in the first cup.  After the water had spilled out it was weighed, which was 8.3g, converted to kg was .0083g.  The weight of this displaced water in Newtons was 0.081423n.
	The percentage error with the buoyant force from step one was calculated using   , this resulted, using .114 for Fb and .0813 for Wdisp, a 28.7% error.
	After completing this lab, it has become more apparent as to how to calculate boyant forces and how that information can be used.
					






Buoyant Forces


 

 


     William H. Gates 
     Chairman and Chief Executive Officer
     Microsoft Corporation 

     William (Bill) H. Gates is chairman and chief executive officer of
     Microsoft Corporation, the leading provider, worldwide, of software
     for the personal computer. Microsoft had revenues of $8.6 billion for
     the fiscal year ending June 1996, and employs more than 20,000
     people in 48 countries.

     Born on October 28, 1955, Gates and his two sisters grew up in
     Seattle. Their father, William H. Gates II, is a Seattle attorney. Their
     late mother, Mary Gates, was a schoolteacher, University of Washington regent and
     chairwoman of United Way International.

     Gates attended public elementary school and the private Lakeside School. There, he began his
     career in personal computer software, programming computers at age 13.

     In 1973, Gates entered Harvard University as a freshman, where he lived down the hall from
     Steve Ballmer, now Microsoft's executive vice president for sales and support. While at
     Harvard, Gates developed the programming language BASIC for the first microcomputer -- the
     MITS Altair.

                   In his junior year, Gates dropped out of Harvard to devote his energies to
                   Microsoft, a company he had begun in 1975 with Paul Allen. Guided by a
                   belief that the personal computer would be a valuable tool on every office
                   desktop and in every home, they began developing software for personal
                   computers. 

                   Gates' foresight and vision regarding personal computing have been central
     to the success of Microsoft and the software industry. Gates is actively involved in key
     management and strategic decisions at Microsoft, and plays an important role in the technical
     development of new products. A significant portion of his time is devoted to meeting with
     customers and staying in contact with Microsoft employees around the world through e-mail.

     Under Gates' leadership, Microsoft's mission is to continually advance and
     improve software technology and to make it easier, more cost-effective and
     more enjoyable for people to use computers. The company is committed to
     a long-term view, reflected in its investment of more than $2 billion on
     research and development in the current fiscal year.

     As of December 12, 1996, Gates' Microsoft stock holdings totaled
     282,217,980 shares.

     In 1995, Gates wrote The Road Ahead, his vision of where information technology will take
     society. Co-authored by Nathan Myhrvold, Microsoft's chief technology officer, and Peter
     Rinearson, The Road Ahead held the No. 1 spot on the New York Times' bestseller list for
     seven weeks. Published in the U.S. by Viking, the book was on the NYT list for a total of 18
     weeks. Published in more than 20 countries, the book sold more than 400,000 copies in China
     alone. In 1996, while redeploying Microsoft around the Internet, Gates thoroughly revised The
     Road Ahead to reflect his view that interactive networks are a major milestone in human
     history. The paperback second edition has also become a bestseller. Gates is donating his
     proceeds from the book to a non-profit fund that supports teachers worldwide who are
     incorporating computers into their classrooms.

                       In addition to his passion for computers, Gates is interested in
                       biotechnology. He sits on the board of the Icos Corporation and is a
                       shareholder in Darwin Molecular, a subsidiary of British-based
                       Chiroscience. He also founded Corbis Corporation, which is developing
                       one of the largest resources of visual information in the world-a
                       comprehensive digital archive of art and photography from public and
                       private collections around the globe. Gates also has invested with
     cellular telephone pioneer Craig McCaw in Teledesic, a company that is working on an
     ambitious plan to launch hundreds of low-orbit satellites around the globe to provide worldwide
     two-way broadband telecommunications service.

     In the decade since Microsoft has gone public, Gates has donated more than $270 million to
     charities, including $200 million to the William H. Gates Foundation. The focus of Gates' giving
     is in three areas: education, population issues and access to technology. 

     Gates was married on Jan. 1, 1994 to Melinda French Gates. They have one child, Jennifer
     Katharine Gates, born in 1996.

     Gates is an avid reader and enjoys playing golf and bridge. 







 
Bugged
   
    In our high tech world, what was once a complicated
electronic task is no longer such a big deal.  I'm talking
about "bugging".  No, I don't mean annoying people; I mean
planting electronic listening devices for the purpose of
eavesdropping.  Bugging an office is a relatively simple
process if one follows a few basic steps.
     First, a person needs to select the bug.  There are
many  different types of bugs ranging from the infinity bug
with which you can listen in on a telephone conversation from
over 200 miles away to an electaronic laser beam which can
pick up the vibrations of a person's voice off a window pane.
The infinity bug sells for $1,000 on the black market and the
laser for $895.  Both, however, are illegal.
     Second, one needs to know where to plant the bug.  A bug
can be hidden in a telphone handset, in the back of a desk
drawer, etc.  The important thing to remember is to place the
bug in a spot near where people are likely to talk.  The bug
may be useless if it is planted too far away from
conversations take place.
    Last one needs to know how to plant the bug.  One of the
most common ways is to wire a 9-volt battery to the phone's
own microphone and attaching it to a spare set of wires that
the phone lines normally contain.  This connection enables
the phone to be live on the hook, sending continuous room
sounds to the eavesdropper.
     It used to be that hidden microphones and concealed tape
recorders were strictly for cops and spies.  Today such
gadgets have filtered down to the jealous spouse, the nosy
neighbor, the high-level executive, and the local politician.
 


 




INTEL Knows Best? 
 A Major Marketing Mistake
Problem Statement
	When Thomas Nicely, a mathematician at Lynchburg College in Virginia, first went public with the fact that Intel's new Pentium chip was defective Intel admitted to the fact that it had sold millions of defective chips, and had known about the defective chips for over four
months.  Intel said its reasoning for not going public was that most people would never encounter any problems with the chip.  Intel said that a spreadsheet user doing random calculations would only have a problem every 27,000 years, therefore they saw no reason to replace all of the defective chips.  However if a user possessed a defective chip and could convince Intel that his or her calculations were particularly vulnerable to the flaw in the defective chip then Intel it would supply those people with a new chip.   This attitude of 'father knows best' fostered by Intel created an uproar among users and owners of the defective chips.   Six weeks after Mr. Nicely went public, IBM, a major purchaser of Pentium chips, stopped all shipments of computers containing the defective Pentium chips.   Intel's stock dropped 5% following this bold move by IBM.  IBM's main contention was that it puts its customers first, and Intel was failing to do this.   
	Intel's handling of this defective chip situation gives rise to many questions.  During the course of this paper I will address several of them.  The first of which is how did a company with such a stellar reputation for consumer satisfaction fall into the trap that the customer does not know best?  Secondly, what made this chip defect more of a public issue than other defective products manufactured and sold to the public in the past?  Finally, how did Intel recover from such a mistake?  How much did it cost them and what lessons can other companies learn from Intel's marketing blunder so that they do not make the same mistake?
Major Findings
	Intel is spearheaded by a chief executive named Andrew Grove.  Grove is a "tightly wound engineering Ph.D. who has molded the company in his image.  Both the secret of his success and the source of his current dilemma is an anxious management philosophy built around the motto 'Only the paranoid survive'."   However, even with this type of philosophy the resulting dominance he has achieved in the computer arena cannot be overlooked.  Intel  practically dominates the computer market with $11.5 billion in sales.  Intel has over 70% of the $11 billion microprocessor market, while it's Pentium and 486 chips basically control the IBM-compatible PC market.  All of these factors have resulted in an envious 56% profit margin that only Intel can seem to achieve.   So what did Intel do to achieve this sort of profit margin?
	In mid-1994 Intel launched a $150m marketing campaign aimed at getting consumers to recognize the Pentium name and the "Intel Inside" logo.  In order to achieve this goal of  brand recognition Intel advertised its own name in conjunction with the "Intel Inside" logo and stated 'with Intel Inside, you know you have got. . . unparalleled quality'.   This provided immediate name recognition for the company and led the consumers to associate Intel with high quality computers.  Then Intel went the extra mile in the marketing world and spent another $80m to promote its new Pentium chips.  The basis for this extra $80m was to "speed the market's acceptance of the new chip".   The marketing campaign was a success.  Intel had managed to achieve brand recognition.  "Once the products were branded, companies found that they could generate even higher sales by advertising the benefits of their products.  This advertising led consumers to regard brands as having very human personality traits, with one proving fundamental to brand longevity  --  trustworthiness."   Consumers readily identified a quality, up to date computer as one with a Pentium chip and the 'Intel Inside' logo stamped on the front.  This "push" marketing strategy of Intel totally dominated the market, thus forcing the Pentium chip to the forefront of the computer market, all at the expense of the cheaper 486.   This "push strategy" of Intel made it plainly clear to its purchasers that Intel was looking out for number one first and its purchasers such as Compaq and IBM second.   Making the Pentium chip the mainstay of the computer industry was the goal of Intel, but a goal that would later come back to haunt them for a brief period of time.
	Throughout the history of the computer industry many manufacturers have sold defective products.  According to Forbes journalist Andrew Kessler, "Every piece of hardware and software ever shipped had a bug in it.  You better get used to it."   Whether or not 'every' piece ever shipped has had a bug is debatable, but there have been numerous examples of valid software bugs.  For example Quicken 3.0 had a bug that resulted in the capitalizing of the second letter of a name incorrectly.  Intuit, however, handled the situation by selling an upgraded version (Quicken 4.0) which fixed the problem, and left the consumer feeling as though he or she had gotten an upgraded version of the existing program.  In essence Intuit had not labeled the upgrade as a debugging program, therefore it had fixed the problem and satisfied the customer all at the same time.  While Intuit's customers were  feeling as though they had a better product by buying the upgrade, Intuit was padding its pocket books through all of the upgrade sales.  Other examples of  companies standing behind their products are in the news week after week.  Just a few years ago Saturn, the GM subsidiary, sent thousands of cars to the junkyards for scrap metal due to corroded engines, a result of  contaminated engine coolant.     Johnson & Johnson, the maker of Tylenol, recalled every bottle of  medicine carrying the Tylenol name and offered a  100% money back guarantee to anyone who had purchased a bottle that might be contaminated.   The precedence was already set, so why would a company with the reputation of Intel fail to immediately replace all of the defective chips it had sold?  Furthermore, why did Intel not come forth immediately when it first discovered that its chips had a problem?
	Intel's engineers said that the defective chips  would affect only one-tenth of 1% of all users, and those users would be doing floating-point operations.   (Floating point operations utilize a matrix of precomputed values, similar to those found in the back of your 1040 tax booklet.  If the values in the table are correct then you will come up with a correct answer.  This was not the case with the Pentium.  A table containing 1066 entries had five incorrect entries, resulting in certain calculations made by the Pentium chips to be inaccurate as high as the fourth significant digit.)   Considering the low number of people that the chip would supposedly affect and the high cost ($475m) associated with replacing the chips, Intel decided a case by case replacement policy "for those limited users doing critical calculations".    Intel's VP-corporate marketing director, Dennis Carter, stated, "We're satisfied that it's addressing the real problem.  From a customer relations standpoint, this is clearly new territory for us.  A recall would be disruptive for PC users and not the right thing to for the consumer".    This policy infuriated the millions of Pentium purchasers who had bought a PC with a Pentium chip.   Word spread like wildfire throughout the consumer world that Intel had sold a defective product and was now refusing to replace it.  This selective replacement policy  is a "classic example of a product driven company that feels its technical expertise is more important than buyers' feelings".   Intel was faced with a decision.  Should they take the attitude of brand is most important and we will take all necessary action to preserve it or take the attitude of what would be the monetary cost of doing the right thing and replacing all of the defective chips, and would it be worth it?  Initially they decided that the monetary cost of replacing all defective chip would not be cost efficient due to the sheer numbers involved.  Intel had sold an estimated 4.5 million Pentium chips worldwide, and approximately 1.9 million in the U.S. alone.   Intel later reversed its selective replacement policy (Intel knows best attitude) and came out with a 100% replacement policy.  What was the reasoning behind this change of attitude at Intel?
	As a result of the selective replacement policy, IBM announced it would stop all shipments of PCs containing the flawed chips.  This combined with the public outcry at having spent thousands of dollars for PCs that did not work as advertised, and the reluctance of corporate users of PCs to purchase new computers resulted in Intel changing its public policy concerning the defective chips.   Intel's new policy was to offer a 100% replacement policy to anyone who desired a new chip.    This policy entailed either sending replacement chips to those users who wanted to replace the chip themselves, or providing free professional replacement of the chip for those who did not feel comfortable doing it themselves.  Intel's new policy was in line with public expectations, but it had been delayed for several precious weeks.  So one might ask, "What did this delayed change in attitude cost Intel in terms of dollars and repeat customers?"
	The resulting costs to Intel were enormous in some respects, but almost negligible in others.  Intel's fourth-quarter earnings were charged $475m for the costs of replacing and writing off the flawed chips.   This was 15% more than analysts had predicted.  Fourth-quarter profits dropped 37% to $372m.   This was a sharp drop in profits, but $372m is still a number to be reckoned with in the fast paced industry of computers.  So did this drop in profits mean that Intel was losing its edge?  I tend to think not, since Intel reported that the sale of Pentiums had doubled between the third and  fourth quarters, thus lifting revenues in 1994 to $11.5 billion, a 31% increase.   Apparently consumers rallied around the new replacement policy and continued to purchase the Pentium equipped computers at a very fast rate, despite the initial reaction of Intel towards replacing the defective chips.  This renewed faith was not regained overnight, but nevertheless it happened, therefore Intel is unlikely to lose its commanding lead in the industry.   So what type of assurance was it that led to this renewed faith in Intel?
	Following Intel's announcement of its 100% replacement policy for the defective chips it recalculated its replacement policy on all future defective products.  Intel realized that its "fatal flaw was adopting a 'father knows b 




 
 I understand that some students that have already graduated from College are
 having a bit of trouble getting their new businesses started.  I know of a tool that will
 be extremely helpful and is already available to them; the Internet. Up until a few years
 ago, when a student graduated they were basically thrown out into the real world with just
 their education and their wits.  Most of the time this wasn't good enough because after
 three or four years of college, the perspective entrepreneur either forgot too much of what
 they were supposed to learn, or they just didn't have the finances.  Then by the time they
 save sufficient money, they again had forgotten too much.  I believe I have found the
 answer. On the Internet your students will be able to find literally thousands of links to
 help them with their future enterprises.  In almost every city all across North America, no
 matter where these students move to, they are able to link up and find everything they
 need.  They can find links like "Creative Ideas", a place they can go and retrieve ideas,
 innovations, inventions, patents and licensing. Once they come up with their own products, 
 they can find free expert advice on how to market their products.  There are easily
 accessible links to experts, analysts, consultants and business leaders to guide their way
 to starting up their own business, careers and lives.  These experts can help push the
 beginners in the right direction in every field of business, including every way to
 generate start up revenue from better management of personal finances to diving into the
 stock market.  When the beginner has sufficient funds to actually open their own company,
 they can't just expect the customers to come to them, they have to go out and attract them.
  This is where the Internet becomes most useful, in advertising.  On the Internet, in every
 major consumer area in the world, there are dozens of ways to advertise.  The easiest and
 cheapest way, is to join groups such as "Entrepreneur Weekly".  These groups offer weekly
 newsletters sent all over the world to major and minor businesses informing them about new
 companies on the market.  It includes everything about your business from what you
 make/sell and where to find you, to what your worth.  These groups also advertise to the
 general public.  The major portion of the advertising is done over the Internet, but this
 is good because that is their target market.  By now, hopefully their business is doing
 well, sales are up and money is flowing in.  How do they keep track of all their funds
 without paying for an expensive accountant?  Back to the Internet.  They can find lots of
 expert advice on where they should reinvest their money.  Including how many and how
 qualified of staff to hire, what technical equipment to buy and even what insurance to
 purchase.  This is where a lot of companies get into trouble, during expansion. Too many
 entrepreneurs try to leap right into the highly competitive mid-size company world.  On the
 Internet, experts give their secrets on how to let their companies natural growth force its
 way in.  This way they are more financially stable for the rough road ahead. The Internet
 isn't always going to give you the answers you are looking for, but it will always lead you
 in the right direction.  That is why I hope you will accept my proposal and make aware the
 students of today of this invaluable business tool.
 



 
	Can computers think?  
The case for and against artificial intelligence

Artificial intelligence has been the subject of many bad '80's
movies and countless science fiction novels.  But what happens when we
seriously consider the question of computers that think.  Is it possible for
computers to have complex thoughts, and even emotions, like homo sapien?  This
paper will seek to answer that question and also look at what attempts are being
made to make artificial intelligence (hereafter called AI) a reality.
	Before we can investigate whether or not computers can think, it is
necessary to establish what exactly thinking is.  Examining the three main
theories is sort of like examining three religions.  None offers enough support so
as to effectively eliminate the possibility of the others being true.  The three main
theories are:  1.  Thought doesn't exist;  enough said.  2.  Thought does exist, but
is contained wholly in the brain.  In other words, the actual material of the brain is
capable of what we identify as thought.  3.  Thought is the result of some sort of
mystical phenomena involving the soul and a whole slew of other unprovable
ideas.  Since neither reader nor writer is a scientist, for all intents and purposes,
we will say only that thought is what we (as homo sapien) experience. 
	So what are we to consider intelligence?  The most compelling
argument is that intelligence is the ability to adapt to an environment.  Desktop
computers can, say, go to a specific WWW address.  But, if the address were
changed, it wouldn't know how to go about finding the new one (or even that it
should).  So intelligence is the ability to perform a task taking into consideration
the circumstances of completing the task.
	So now that we have all of that out of that way, can computers think?  The
issue is contested as hotly among scientists as the advantages of Superman over
Batman is among pre-pubescent boys.  On the one hand are the scientists who say,
as philosopher John Searle does, that "Programs are all syntax and no semantics." 
(Discover, 106)  Put another way, a computer can actually achieve thought
because it "merely follows rules that tell it how to shift symbols without ever
understanding the meaning of those symbols."  (Discover, 106)  On the other side
of the debate are the advocates of pandemonium, explained by Robert Wright in
Time thus:  "[O]ur brain subconsciously generates competing theories about the
world, and only the 'winning' theory becomes part of consciousness.  Is that a
nearby fly or a distant airplane on the edge of your vision?  Is that a baby crying
or a cat meowing?  By the time we become aware of such images and sounds,
these debate have usually been resolved via a winner-take-all struggle.  The
winning theory-the one that best matches the data-has wrested control of our
neurons and thus our perceptual field."  (54)  So, since our thought is based on
previous experience, computers can eventually learn to think.
	The event which brought this debate in public scrutiny was Garry
Kasparov, reigning chess champion of the world, competing in a six game chess
match against Deep Blue, an IBM supercomputer with 32 microprocessors. 
Kasparov eventually won (4-2), but it raised the legitimate question, if a computer
can beat the chess champion of the world at his own game (a game thought of as
the ultimate thinking man's game), is there any question of AI's legitimacy? 
Indeed, even Kasparov said he "could feel-I could smell- a new kind of
intelligence across the table."  (Time, 55)  But, eventually everyone, including
Kasparov, realized that what amounts to nothing more than brute force, while
impressive, is not thought.  Deep Blue could consider 200 million moves a
second.  But it lacked the intuition good human players have.  Fred Guterl,
writing in Discover, explains.  "Studies have shown that in a typical position, a
strong human play considers on average only two moves.  In other words, the
player is choosing between two candidate moves that he intuitively recognizes,
based on prior experience, as contributing to the goals of the position."
	Seeking to go beyond the brute force of Deep Blue in separate
projects, are M.I.T. professor Rodney Brooks and computer scientist Douglas
Lenat.  The desire to conquer AI are where the similarities between the two end.
	Brooks is working on an AI being nicknamed Cog.  Cog has
cameras for eyes, eight 32-bit microprocessors for a brain and soon will have a
skin-like membrane.  Brooks is allowing Cog to learn about the world like a baby
would.  "It sits there waving its arm, reaching for things."  (Time, 57)  Brooks's
hope is that by programming and reprogramming itself, Cog will make the leap to
thinking.  This expectation is based on what Julian Dibbell, writing in Time,
describes as the "bottom-up school.  Inspired more by biological structures than
by logical ones, the bottom-uppers don't bother trying to write down the rules of
thought.  Instead, they try to conjure thought up by building lots of small, simple
programs and encouraging them to interact."  (57)
	Lenat is critical of this type of AI approach.  He accuses Brooks of
wandering aimlessly trying to recreate evolution.  Lenat has created CYC.  An AI
program which uses the top down theory which states that "if you can write down
the logical structures through which we comprehend the world, you're halfway to
re-creating intelligence.  (Time, 57)  Lenat is feeding CYC common sense
statements (i.e. "Bread is food.") with the hopes that it will make that leap to
making its own logical deductions.  Indeed, CYC can already pick a picture of a
father watching his daughter learn to walk when prompted for pictures of happy
people.  Brooks has his own criticisms for Lenat.  "Without sensory input, the
program's knowledge can never really amount to more than an abstract network
of symbols.
	So, what's the answer?  The evidence points to the position that AI is
possible.  What is our brain but a complicated network of neurons?  And what is
thought but response to stimuli?  How to go about achieving AI is another
question entirely.  All avenues should be explored.  Someone is bound to hit on it. 
Thank you. 



 
Five years after the first world wide web was launched at the end of 1991, The Internet has become very popular in the United States. Although President Clinton already signed the 1996 Telecommunication ActI on Thursday Feb 8, 1996, the censorship issue on the net still remains unresolved. In fact, censorship in cyberspace is unconscionable and impossible. Trying to censor the Internet its problematic because the net is an international issue, there is no standard for judging materials, and censorship is an abridgment of democratic spirit.
Firstly, censorship on the Internet is an international issue. The Internet was constructed by the U.S. military since 1960s, but no one actually owns it. Thus, the Internet is a global network, and it crosses over different cultures. It is impossible to censor everything that seems to be offensive. For example, Vietnam has announced new regulations that forbid "data that can affect national security, social order, and safety or information that is not appropriate to the culture, morality, and traditional customs of the Vietnamese people." on June 4, 1996. It is also impossible to ban all things that are prohibited in a country. For instant, some countries, such as Germany, have considered taking measures against the U.S. and other companies or individuals that have created or distributed offensive material on the Internet. If the United States government really wanted to censor the net, there is only one solution - shut down all network links of other countries. But of course that would mean no Internet access for the whole country and that is disgust by the whole nation.
Secondly, everyone has their personal judgment values. The decision of some people cannot represent the whole population of those using the net. Many people debate that pornography on the net should be censored because there are kids online. However, we can see there are many kids of pornographic magazines on display at newsstands. It is because we have regulations to limit who can read certain published materials. Likewise, some people already use special software to regulate the age limit in cyberspace. Why do people still argue about that? It is all about personal points of views. Justice Douglas said, "To many the Song of Solomon is obscene. I do not think we, the judges, were ever given the constitutional power to make definitions of obscenity."II. In cyberspace, it is hard to set up a pool of judges to censor what could be displayed on the net.
Thirdly, censorship works against democratic spirit, it opposes the right of free speech and is a breach of the First Amendment. Do you remember Salman Rushdie and his book The Satanic Verses? Iranian government announced a death threat to kill Rushdie and his publishers because his book speaks against Islam. No one  wants that to happen again. If you are one of the Internet users, you should have seen a blue ribbon logo. The blue ribbon symbolizes a support for the essential human right of free speech. Let think about what happen if we lost the right of free speech. How can we stay online? Who gives courage to the web's designers to put their opinion on the net? On the same day when the 1996 Telecommunication Act signed in law, a bill called House Bill 1630 was introduced by Georgia House of Representatives member Don Parsons. It is so repel that this law even limits the right of choosing email addressesIII. "Freedom of speech on the Internet deserves the same protection as freedom of the press, freedom of speech, or freedom of assembly." said Bill GatesIV.
In addition, information in cyberspace can be changing from second to second. If you put something on the web, everyone on the net can access it instantly. It is totally different from all traditional media. Everything on the Internet is just a combination of zero and oneV. It is very difficult to chase what has been published on the information superhighway.
After President Clinton signed the 1996 Telecommunication Act, lots of net users reacted in outrage. Although the Federal court in Philadelphia and New York have overturned that Act, The government has appealed the ruling and the case has been referred to the U.S. Supreme Court. Since censorship is an international issue, people have different judgment and censorship works against the democratic spirit. Censorship in the Internet is totally unacceptable. According Justice Potter Stewart's words, "Censorship reflects a society's lack of confidence in itself. It is a hallmark of an authoritarian regime. Long ago those who wrote our First Amendment charted a different course. They believed a society can be truly strong only when it is truly free.VI". If we allow those few in society to censor whatever they find offensive, we have forfeited our right of freedom and have lost our power as a democratic nation.

I.)  On Thursday Feb 1, 1996, Congress approved legislation to dramatically restrict the First Amendment rights of Internet users. President Clinton signed into law Thursday Feb. 8, 1996
II.)  Miller v. California, 413 U.S. 15, 46 (1973), Justice Douglas, dissenting opinion.
III.)  The bill makes it illegal for email users to have addresses that do not include their own names.
IV.)   Bill Gates, Microsoft Magazine Volume 3 Issue 4 Page 54, TPD Publishing Inc., 1996
V.)  The way in which computers read data.
VI.)   Ginzburg v. United States, 383 U.S. 463, 498 (1966) 


 
By Clifford Stoll

"The Cuckoo's Egg" is a story of persistence, love for one's work and is just plain funny!  The story starts out with Clifford Stoll being "recycled" to a computer analyst/webmaster.  
Cliff, as he is affectionately called, is a long-haired ex-hippie that works at Lawrence Berkeley Lab.  He originally was an astronomer, but since his grant wore out, he became a mainframe master.  He was glad that instead of throwing him out into the unemployment office, the Lab recycled their people and downstairs he went, to the computer lab.  
A few days after he becomes the master of the mainframe, his colleague, Wayne Graves, asks him to figure out a 75cent glitch that is in the accounting system.  It turns out that a computer guru, "Seventek" seems to be in town.  None of his closest friends know that.  The Lab becomes suspicious that it might be a hacker.  To fill you in who Seventek is, he is a computer guru that created a number of programs for the Berkeley UNIX system.  At the time, he was in England far from computers and civilization.  The crew does not what to believe that it would be Seventek, so they start to look what the impostor is doing.  Cliff hooks up a few computers to the line that comes from the Tymnet.  Tymnet is a series of fiber-optic cables that run from a major city to another major city.  So if you were in LA and wanted to hook up to a computer in the Big Apple you could call long distance, have a lot of interference from other callers and have a slow connection, or you could sign-up to Tymnet and dial locally, hop on the optic cable and cruise at a T-3 line.  The lab had only five Tymnet lines so Cliff could easily monitor every one with five computers, teletypes, and five printers.  That was the difficult part, where to get all that equipment.  At graduate school they taught Cliff to improvise.  It was a Friday, and not many people come to work on Saturday.  Since it was easier to make up an excuse than to beg for anything, he "borrowed" everything he needed.  Then programmed his computer to beep twice when someone logged on from the Tymnet lines.  The thing is, since he was sleeping under his desk, he would gouge his head on the desk drawer.  Also, many people like to check their E-mail very late at night, so not to get interference. Because of that his terminal beeped a lot!  The next day, he was woken up by the cable operator.  Cliff said that he must have smelled like a dying goat.  Any way, the hacker only logged on once during the night, but left an 80 foot souvenir behind.  Cliff estimated a two to three hours roaming through the three million dollar pieces of silicon that he calls a computer.  During that time he planted a "Cuckoo's egg." 
 The cuckoo is a bird that leaves its eggs in other bird's nest.  If it not were for the other species ignorance, the cuckoo would die out.  The same is for the mainframe.  There is a housecleaning program that runs every five minutes on the Berkeley UNIX.  It is called atrun.  The hacker put his version of atrun into the computer through a hole in the Gnu-Emacs program.  It is a program that lets the person who is sending E-mail put a file anywhere they wished.  So that is how the hacker became a "Superuser."  A superuser has all the privileges of a system operator, but from a different computer.  Cliff called the FBI, the CIA, and all the other three lettered agencies that that had spooks in trench coat and dark glasses (and some of them had these nifty ear pieces too!)  Everyone except the FBI lifted a finger. The FBI listened but, they stated that if they hadn't lost millions of dollars in equipment, or classified data, they didn't what to know them.  The hodgepodge of information between the CIA, NSA, and Cliff began to worry his lover, Martha.  A little background on her.  She and Clifford have know each other since they were kids, and lovers since they turned adults.  They didn't feel like getting married because they thought that was a thing that you do when you're very bland.  They wanted freedom. If they ever wanted to leave they would just pack their bags, pay their share of the utilities and hightail it out of there.  Well back to the plot. She too was an ex-hippie and she hated anything that had to do with government.  The spook calls were killing their relationship.
  When Cliff wanted to trace a phone call to the hacker, the police said. "That just isn't our bailiwick."  It seemed that everyone wanted information, wanted Cliff to say open with his monitoring system, but nobody seemed interested in paying for the things that were happening.  
When Cliff found the hacker in a supposedly secure system, he called the system administrator.  The hacker was using the computer in their system to dial anywhere he wished, and they picked up the tab.  The guy was NOT happy.  He asked if he was to close up shop for the hacker and change all the passwords.  Cliff answered no, he wanted to track the guy/gal.  First Cliff strategically master minded a contrivance.  He would ask for the secure system's phone records, which would show him (theoretically) where the hacker is calling to.  Then that night, Cliff became the hacker.  He used his computer to log in to his account at Berkley and then he would Telnet to the hacked system, try the passwords and see what he could see.  Boy was he ever surprised!  He could call anywhere, for free!!  He had access to other computer on the network also, one sensitive at that.  
The next day, Cliff called the sys administrator, and told him about his little excursion.  The guy answered.  "Sorry Cliff, we have to close up shop.  This went right up the line, and well, the modems are going down for a long time."  This irritated Clifford.  He was so close!  Anyway his life went back to semi-normal. (Was it ever?!)  Then unexpectedly his beeper beeped.  To fill you in, he got him self a beeper for those unexpected pleasures.  He was in the middle of making scrambled eggs for Martha, who was still asleep.  He wrote her a note saying "The case is afoot!!J", leaving the eggs still in the pan.  
The hacker didn't come through the now secure system, but through another line, over Tymnet.  He called Tymnet and got them to do a search.  They traced him over the "puddle" (the Atlantic) to the German Datex Network.  They couldn't trace any further because the German's network is all switches, not like the computerized switches of the good ol' US of A!  There would have to be a technician, there tracing the wire along the wall, into the ground, and maybe on to a telephone pole.  Not only that, the Germans wouldn't do anything without a search warrant.  
Every minor discovery was told about six times to the different three letter agencies that were on the case.  Mean while, since this was no longer a domestic case, and was remotely interesting for the FBI, they took the case, out of pure boredom.  
The CIA affectionately called the FBI the "F entry".  Now that the guys at the F entry were in, there was work to be done.  They got a warrant, but the guy who was to deliver, never did. This was beginning to be serious.  Every time Cliff tried to get some info on what is going on across the puddle, the agencies clamed up.  
When the warrant finally came, the Germans let the technicians be there to midnight German time.  As soon as the fiend on the other side raised his periscope, they would nail him.  
The problem was, to trace him, well, he needed to be on the line for about two hours!  The kicker is that he was on for mostly two to three minute intervals.  That is when Operation Showerhead came into effect!!  Martha came up with this plan while in the shower with Cliff...First make up some cheesy files that sound remotely interesting.  Then place them in a spot that only he and the hacker could read.  Recall that the hacker was after military files.  They take files that were all ready there, change all the Mr. to General, all the Ms to corporal and all the Professors to Sergeant Major.  All that day they made up those files.  Then they pondered what the title should be, STING or SDINET.  They chose SDINET because STING looked too obvious.  Then they created a bogus secretary, under the address of a real one.  Cliff put enough files on the directory so that it would take the hacker at least three hours of dumping the whole file onto his computer.
In one of the files it said that if you wanted more info, send to this address.  Well one day, Cliff was actually doing some work, for a change, when the real secretary called to say that a letter came for the bogus secretary.  Cliff ran up the stairs, the elevator was too slow.  They opened it and she read it aloud to Cliff who was in utter amassment.  Then he called the F entry.  They told him not to touch the document and to send to them in a special envelope.  He did. 
Cliff was at home one day and all of a sudden his beeper beeped.  Since he programmed it to beep in Morse code, he knew where the hacker was coming from before he physically saw him on the screen.  Martha groaned while Clifford jumped on his old ten speed and rode to work.  When he got there, the hacker just started to download the SDINET files from the UNIX.  He called Tymnet and started the ball rolling.  That day the hacker was on for more than two hours, enough for the trace to be completed.  Though he knew that the FBI knew the number, they wouldn't tell him who the predator was.  
For the next few days, Clifford expected to get a call from the Germans saying, "You can close up your system, we have him at the police station now."  That didn't happen.  He got word, though, that there was a search of his home, and they recovered printouts, computer back-up tapes, and disks, and diskettes.  That was enough evidence to lock him up for a few years.  Then one day, they caught him in the act.  That was enough, he was in the slammer awaiting trail.  
Clifford's adventure was over, he caught his hacker, and was engaged to Martha.  They decided to get married after all.  He returned to being an astronomer, and not a computer wizard.  Though many people though of him as a wizard, he himself though that what he did was a discovery that he stumbled on.  From a 75cent accounting mishap to Tymnet to Virginia, to Germany.  What a trace!  
At the end of the story, poor Cliff was sobbing because he grew up!! L  To him that was a disaster, but the wedding coming up, and his life officially beginning, he forgot it soon.  Now he lives in Cambridge with his wife, Martha, and three cats that he pretends to dislike.  
  
CMIP vs. SNMP : Network Management

     Imagine yourself as a network administrator, responsible for a 2000 user network.  
This network reaches from California to New York, and some branches over seas.  In 
this situation, anything can, and usually does go wrong, but it would be your job as a 
system administrator to resolve the problem with it arises as quickly as possible.  The 
last thing you would want is for your boss to call you up, asking why you haven't done 
anything to fix the 2 major systems that have been down for several hours.  How do 
you explain to him that you didn't even know about it? Would you even want to tell 
him that?  So now, picture yourself in the same situation, only this time, you were 
using a network monitoring program.  Sitting in front of a large screen displaying a 
map of the world, leaning back gently in your chair. A gentle warning tone sounds, and 
looking at your display, you see that California is now glowing a soft red in color, in 
place of the green glow just moments before.  You select the state of California, and it 
zooms in for a closer look.  You see a network diagram overview of all the computers 
your company has within California.  Two systems are flashing, with an X on top of 
them indicating that they are experiencing problems. Tagging the two systems, you 
press enter, and with a flash, the screen displays all the statitics of the two systems, 
including anything they might have in common causing the problem.  Seeing that both 
systems are linked to the same card of a network switch, you pick up the phone and 
give that branch office a call, notifying them not only that they have a problem, but 
how to fix it as well.   
     Early in the days of computers, a central computer (called a mainframe) was 
connected to a bunch of dumb terminals using a standard copper wire.  Not much 
thought was put into how this was done because there was only one way to do it: they 
were either connected, or they weren't.  Figure 1 shows a diagram of these early 
systems. If something went wrong with this type of system, it was fairly easy to 
troubleshoot, the blame almost always fell on the mainframe system.
     Shortly after the introduction of Personal Computers (PC), came Local Area 
Networks (LANS), forever changing the way in which we look at networked systems.  
LANS originally consisted of just PC's connected into groups of computers, but soon 
after, there came a need to connect those individual LANS  together forming what is 
known as a Wide Area Network, or WAN, the result was a complex connection of 
computers joined together using various types of interfaces and protocols.  Figure 2 
shows a modern day WAN.  Last year, a survey of Fortune 500 companies showed that 
15% of their total computer budget, 1.6 Million dollars, was spent on network 
management (Rose, 115).  Because of this, much attention has focused on two families 
of network management protocols: The Simple Network Management Protocol 
(SNMP), which comes from a de facto standards based background of TCP/IP 
communication, and the Common Management Information Protocol (CMIP), which 
derives from a de jure standards-based background associated with the Open Systems 
Interconnection (OSI) (Fisher, 183).  
 

     In this report I will cover advantages and disadvantages of both Common 
Management Information Protocol (CMIP) and Simple Network Management Protocol 
(SNMP)., as well as discuss a new protocol for the future.  I will also give some good 
reasons supporting why I believe that SNMP is a protocol that all network 
administrators should use. 
     SNMP is a protocol that enables a management station to configure, monitor, and 
receive trap (alarm) messages from network devices. (Feit, 12).  It is formally specified 
in a series of related Request for Comment (RFC) documents, listed here.
	RFC 1089 - SNMP over Ethernet
    	RFC 1140 - IAB Official Protocol Standards
    	RFC 1147 - Tools for Monitoring and Debugging TCP/IP
            	        Internets and Interconnected Devices
           	    	        [superceded by RFC 1470]
    	RFC 1155 - Structure and Identification of Management
             	        Information for TCP/IP based internets.
    	RFC 1156 - Management Information Base Network
              	        Management of TCP/IP based internets
    	RFC 1157 - A Simple Network Management Protocol
  	RFC 1158 - Management Information Base Network
         	                   Management of TCP/IP based internets: MIB-II
   	RFC 1161 - SNMP over OSI
   	RFC 1212 - Concise MIB Definitions
	RFC 1213 - Management Information Base for Network Management
   	                   of TCP/IP-based internets: MIB-II
  	RFC 1215 - A Convention for Defining Traps for use with the SNMP
    	RFC 1298 - SNMP over IPX (SNMP, Part 1 of 2, I.1.) 
    The first protocol developed was the Simple Network Management Protocol 
(SNMP).  It was commonly considered to be a quickly designed "band-aid" solution to 
internetwork management difficulties while other, larger and better protocols were 
being designed. (Miller, 46).  However, no better choice became available, and SNMP 
soon became the network management protocol of choice.
     It works very simply (as the name suggests):  it exchanges network packets through 
messages (known as protocol data units (PDU)).  The PDU contains variables that 
have both titles and values.  There are five types of  PDU's which SNMP uses to 
monitor a network: two deal with reading terminal data, two with setting terminal data, 
and one called the trap, used for monitoring network events, such as terminal start-ups 
or shut-downs.
     By far the largest advantage of SNMP over CMIP is that its design is simple, so it is 
as easy to use on a small network as well as on a large one, with ease of setup, and lack 
of stress on system resources.  Also, the simple design makes it simple for the user to 
program system variables that they would like to monitor.  Another major advantage to 
SNMP is that is in wide use today around the world.  Because of it's development 
during a time when no other protocol of this type existed, it became very popular, and 
is a built in protocol supported by most major vendors of networking hardware, such as 
hubs, bridges, and routers, as well as majoring operating systems. It has even been put 
to use inside the Coca-Cola machines at Stanford University, in Palo Alto, California 
(Borsook, 48).  Because of SNMP's smaller size, it has even been implemented in 
such devices as toasters, compact disc players, and battery-operated barking dogs.  In 
the 1990 Interop show, John Romkey, vice president of engineering for Epilogue, 
demonstrated that through an SNMP program running on a PC, you could control a 
standard toaster through a network (Miller, 57).
     SNMP is by no means a perfect network manager.  But because of it's simple 
design, these flaws can be fixed.  The first problem realized by most companies is that 
there are some rather large security problems related with SNMP.  Any decent hacker 
can easily access SNMP information, giving them any information about the network, 
and also the ability to potentially shut down systems on the network.  The latest version 
of SNMP, called SNMPv2, has added some security measures that were left out of 
SNMP, to combat the 3 largest problems plaguing SNMP:  Privacy of Data (to prevent 
intruders from gaining access to information carried along the network), authentication 
(to prevent intruders from sending false data across the network), and access control 
(which restricts access of particular variables to certain users, thus removing the 
possibility of a user accidentally crashing the network). (Stallings, 213)
     The largest problem with SNMP, ironically enough, is the same thing that made it 
great; it's simple design.  Because it is so simple, the information it deals with is 
neither detailed, nor well organized enough to deal with the growing networks of the 
1990's.  
This is mainly due to the quick creation of SNMP, because it was never designed to be 
the network management protocol of the 1990's.  Like  the previous flaw, this one too 
has been corrected with the new version, SNMPv2.  This new version allows for more 
in-detail specification of variables, including the use of the table data structure for 
easier data retrieval.  Also added are two new PDU's that are used to manipulate the 
tabled objects.  In fact, so many new features have been added that the formal 
specifications for SNMP have expanded from 36 pages (with v1) to 416 pages with 
SNMPv2. (Stallings, 153) Some people might say that SNMPv2 has lost the simplicity, 
but the truth is that the changes were necessary, and could not have been avoided. 
     A management station relies on the agent at a device to retrieve or update the 
information at the device. The information is viewed as a logical database, called a 
Management Information Base, or MIB. MIB modules describe MIB variables for a 
large variety of device types, computer hardware, and software components.  The 
original MIB for Managing a TCP/IP internet (now called MIB-I) was defined in RFC 
1066 in August of 1988.  It was updated in RFC 1156 in May of 1990.  The MIB-II 
version published in RFC 1213 in May of 1991, contained some improvements, and 
has proved that it can do a good job of meeting basic TCP/IP management needs.  
MIB-II added many useful variables missing from MIB-I (Feit, 85).  MIB files are 
common variables used not only by SNMP, but CMIP as well.
     In the late 1980's a project began, funded by governments, and large corporations. 
Common Management Information Protocol (CMIP) was born.  Many thought that 
because of it's nearly infinite development budget, that it would quickly become in 
widespread use, and overthrow SNMP from it's throne.  Unfortunately, problems with 
its implementation have delayed its use, and it is now only available in limited form 
from developers themselves. (SNMP, Part 2 of 2, III.40.)
     CMIP was designed to be better than SNMP in every way by repairing all flaws, 
and expanding on what was good about it, making it a bigger and more detailed 
network manager.  It's design is similar to SNMP, where PDU's are used as variables 
to monitor the network.  CMIP however contains 11 types of PDU's (compared to 
SNMP's 5).  In CMIP, the variables are seen as very complex and sophisticated data 
structures with three attributes.  These include:
1)  Variable attributes: which represent the variables characteristics (its data 
type, whether it is writable)
2)  variable behaviors: what actions of that variable can be triggered.
3)  Notifications: the variable generates an event report whenever a specified 
event occurs (eg. A terminal shutdown would cause a variable notification 
event) (Comer, 82)
As a comparison, SNMP only employs variable properties from one and three above.
The biggest feature of the CMIP protocol is that its variables not only relay information 
to and from the terminal (as in SNMP) , but they can also be used to perform tasks that 
would be impossible under SNMP.  For instance, if a terminal on a network cannot 
reach the fileserver a pre-determined amount of times, then CMIP can notify 
appropriate personnel of the event.  With SNMP however, a user would have to 
specifically tell it to keep track of unsuccessful attempts to reach the server, and then 
what to do when that variable reaches a limit. CMIP therefore results in a more 
efficient management system, and less work is required from the user to keep updated 
on the status of the network.  CMIP also contains the security measures left out by 
SNMP.  Because of the large development budget, when it becomes available, CMIP 
will be widely used by the government, and the corporations that funded it.
     After reading the above paragraph, you might wonder why, if CMIP is this 
wonderful, is it not being used already? (after all, it had been in development for nearly 
10 years)  The answer is that possibly CMIP's only major disadvantage, is enough in 
my opinion to render it useless.  CMIP requires about ten times the system resources 
that are needed for SNMP.  In other words, very few systems in the world would able 
to handle a full implementation on CMIP without undergoing massive network 
modifications.  This disadvantage has no inexpensive fix to it.  For that reason, many 
believe CMIP is doomed to fail.  The other flaw in CMIP is that it is very difficult to 
program.  Its complex nature requires so many different variables that only a few 
skilled programmers are able to use it to it's full potential.  
     Considering the above information, one can see that both management systems have 
their advantages and disadvantages.  However the deciding factor between the two, 
lies with their implementation, for now, it is almost impossible to find a system with 
the necessary resources to support the CMIP model, even though it is superior to 
SNMP (v1 and v2) in both design and operation.  Many people believe that the 
growing power of modern systems will soon fit well with CMIP model, and might 
result in it's widespread use, but I believe by the time that day comes, SNMP could 
very well have adapted itself to become what CMIP currently offers, and more.  As 
we've seen with other products, once a technology achieves critical mass, and a 
substantial installed base, it's quite difficult to convince users to rip it out and start 
fresh with an new and unproven technology (Borsook, 48).  It is then recommend that 
SNMP be used in a situation where minimial security is needed, and SNMPv2 be used 
where security is a high priority.

 

Works Cited
Borsook, Paulina.  "SNMP tools evolving to meet critical LAN needs." Infoworld 
     June 1, 1992: 48-49.
Comer, Douglas E.  Internetworking with TCP/IP New York: Prentice-Hall,
      Inc., 1991.
Dryden, Partick.  "Another view for SNMP."  Computerworld December 11, 1995: 12.
Feit, Dr. Sidnie.  SNMP. New York:  McGraw-Hill Inc., 1995.
Fisher, Sharon.  "Dueling Protocols." Byte March 1991: 183-190.
Horwitt, Elisabeth.  "SNMP holds steady as network standard." Computerworld 
     June 1, 1992: 53-54.
Leon, Mark.  "Advent creates Java tools for SNMP apps." Infoworld 
     March 25, 1996: 8.
Marshall, Rose.  The Simple Book.  New Jersey: Prentice Hall, 1994.
Miller, Mark A., P.E.  Managing Internetworks with SNMP New York: M&T 
     Books, 1993. 
Moore, Steve.  "Committee takes another look at SNMP." Computerworld 
     January 16, 1995: 58.
Moore, Steve.  "Users weigh benefits of DMI, SNMP." Computerworld 
     July, 31 1995:  60.
The SNMP Workshop & Panther Digital Corporation.  SNMP FAQ Part 1 of 2.     
     Danbury, CT: http://www.www.cis.ohio-state.edu/hypertext/faq/usenet/snmp-   
     faq/part1/faq.html, pantherdig@delphi.com.
The SNMP Workshop & Panther Digital Corporation.  SNMP FAQ Part 2 of 2. 
     Danbury, CT: http://www.www.cis.ohio-state.edu/hypertext/faq/usenet/snmp-  
     faq/part2/faq.html, pantherdig@delphi.com.
Stallings, William.  SNMP, SNMPv2, and CMIP. Don Mills, Addison-Wesley, 1993.
Vallillee, Tyler, web page author.   Http://www.undergrad.math.
     uwaterloo.ca/~tkvallil/snmp.html
VanderSluis, Kurt.  "SNMP: Not so simple."  MacUser October 1992: 237-240

12
 
 	The article on Cognitive Artifacts by David A. Norman deals with the theories and principles of artifacts as they relate to the user during execution and completion of tasks. These principles and theories that Norman speaks about may be applied to any graphical user interface, however I have chosen to relate the article to the interface known as Windows 95.    
Within Windows 95, Microsoft has included a little tool called the wizard that guides us through the steps involved in setting up certain applications.  This wizard is a very helpful tool to the non experienced computer user, in the way that it acts like a to-do list.  The wizard takes a complex task and breaks it into discrete pieces by asking questions and responding to those questions based on the answers.  Using Norman's theories on system view and the personal view of artifacts, we see that the system views the wizard as an enhancement.  For example, we wanted to set up the Internet explorer, you click on the icon answer the wizard's questions and the computer performs the work.  Making sure everything is setup properly without the errors that could occur in configuring the task yourself.  The wizard performs all the functions on its little to-do list without having  the user worrying about whether he/she remembered to include all the commands.  On the side of personal views the user may see the wizard as a new task to learn but in general it is simpler than having to configure the application yourself and making an error, that could cause disaster to your system.  The wizard also prevents the user from having to deal with all the internal representation of the application like typing in command lines in the system editor. 
	Within Windows 95 most of the representation is internal therefore we need a way to transform it to surface representation so it is accessible to the user.  According to Norman's article there are "three essential ingredients in representational systems.  These being the world which is to be represented, the set of symbols representing the world, and an interpreter."   This is done in Windows by icons on the desktop and on the start menu.  The world we are trying to represent to the user is the application, which can be represented by a symbol which is the icon.  These icons on the desktop and on the start menu are the surface representations the user sees when he goes to access the application not all the files used to create it or used in conjunction with the applications operation.  With the icons a  user can retrieve applications and their files by a click of a button.  The icons lead the user directly into the application without showing all the commands the computer goes through to open the application.  The icons make the user more efficient in accomplishing tasks because it cuts done on the time of trying to find an item when the user can relate what he/she wants to do by the symbol on the icon.
Another example of an artifact within Windows 95 that exhibits Norman's theories is the recycle bin.  This requires the user to have a direct engagement with the windows explorer and knowing the right item to delete.  As a user decides that he no longer desires a certain program and chooses to delete the item, he is executing a command that will change the perception of the system.  By selecting the item to delete the user has started an activity flow which involves the gulf of evaluation and the gulf of execution.  Either of these gulfs could be perceived differently by the user then by the system so Windows 95 prompts the user with a dialog box asking if the user is sure he/she wants to remove this item from the system and it prompts again when emptying the recycle bin.  What the user intends to do and what the system plans to do might not be the same so by prompting the user for action we are double checking that this is what the user has in mind.  However when windows prompts us with the confirmation message, we are breaking the scheduled activity flow.  The main problem with halting the activity flow is that it breaks the user's attention, however when deleting an item you could have selected the wrong item by mistake and without the break in activity flow the outcome could be dangerous.  Norman calls these breaks "forcing functions which prevent critical or dangerous actions from happening without conscious attention." 
The artifacts discussed above using Windows 95 graphical user interface are very similar to the theories and principles that Norman suggests in his article.  Norman has stressed that cognitive artifact should follow three aspects which I feel Windows has dealt with.  Windows 95 in itself has been made so that it is adaptable to the user whether he/she be an experienced user or not, by creating artifacts like icons and menu bars that are all related to one another.  This makes it easier for the user to adapt to its environment and continue computing happily. 
 
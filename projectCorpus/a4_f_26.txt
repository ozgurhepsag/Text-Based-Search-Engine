The system output was classed as "perfect" when correct, and as "best guess" when the correct pronunciation was the closest one to the actual output pattern. The percentage of perfect pronunciation was at 55% and still rising (but very slowly) after 50,000 words of learning. But the percentage of best guesses was higher at 95%.The great strength of this system is that it employs a well-defined link-adjustment process called back propagation which is also guaranteed to continually decrease overall error. The outcome is a system that after many thousands of trials (i.e. input a sequence, check output, and adjust link weights) can be heard to switch from random babbling, through very alien pronunciation, to achieve finally quite good pronunciation들.e. it learns to pronounce English correctly. This is particularly remarkable because no pronunciation knowledge is explicitly built into the network든ach network unit is initially connected to all units in the next layer and link weights are initialized by a random procedure. Yet the networks learns to pronounce English from feedback of the errors it makes. This sounds a bit like magic, and perhaps just what the AI software engineer needs, but all is not sweetness and light (as you no doubt guessed).There are considerable drawbacks. First, you have to know all the right answers and then the system can be trained to exhibit this known functionality듮his is not open-ended learning. Secondly, the conceptual transparency problem: taking NETtalk as an example, when the network has learned to pronounce English, it must in some sense have learned, and thus contain, the knowledge of how to pronounce English. But within the  Page 194resultant collection of link weights (which is all that is altered by the training algorithm) it is extremely difficult to identify any conceptually meaningful substructures that relate letter contexts to pronunciation patterns. This being the case, there is no way for a software engineer (or knowledge engineer, or even network engineer) to study the competent network and modify it so that the system pronounces English with say an American accent. The only known way to achieve this quite closely related goal is to retrain the network from scratch using correct American pronunciation to generate the error signals.But it is early days yet for the subfield of connectionism, and impossible to predict what may come out of it to support the engineering of practical AI software. It does perhaps provide us with a glimpse of a radically new approach to the engineering of software systems, even if they are 'black-box' systems, and hence my reason for bringing it to your attention.Practical machine learning examplesHaving given you a quick tour of the frontiers of machine learning insofar as it relates to possibilities for practical software, it is time to look at a few of the success stories here.Inductive generalization techniques have been used to generate expert systems from a set of examples of the expertise. Michalski and Chilausky (1980) describe the use of this general technique to generate an expert system for soybean disease diagnosis. Quite surprisingly (as the authors admit), the diagnosis system based on the inductively generated rules was repeatedly and consistently better than a similar system that was built using the traditional approach of codifying the experts' rules. "The major conclusion from this experiment is that the current computer induction techniques can already offer a viable knowledge acquisition method if the problem domain is sufficiently simple and well-defined" Michalski and Chilausky, 1980, p. 79.In another project Quinlan (1987) presents a probabilistic classifier that begins to address the problems of real-world data (e.g. noise and uncertainty). His scheme generates a probabilistic decision tree which "can provide valuable information on the probability that a case belongs to one or more of the classes, and is less brittle with respect to unknown or imprecise attribute values and inexact thresholds" p. 37.Figure 7.4 illustrates one such probabilistic decision tree which was generated from a set of 2800 cases of specific symptoms and the  Page 195subsequent diagnosis of thyroid disease. This data was both imperfect and noisy, as is the nature of real-world data. The tree was developed in two stages: a classical induction algorithm which generated a large tree with much complexity caused by the noise in the training set; followed by a process of 'pessimistic pruning' ''which simplified the tree by replacing some subtrees with leaves... This sort of simplification has been found to improve the accuracy when the tree is used to classify unseen cases... A leaf produced by such pruning will generally misclassify some cases in the the training set. If each leaf is identified with the subset of training cases that it covers, some of these subsets will contain representatives from more than one class. This is indicated... by the annotation (n) or (n/e) following each leaf; n is the number of training cases in the subset for that leaf, and e, if it appears, is the number of these cases that do not belong to the class nominated by the leaf" p. 33. Figure 7.4 "A decision tree for hyperthyroidism" from Quinlan, 1987, Figure 1, p. 33  Page 196These leaf annotations then provide a measure of the accuracy of the leaves, which each represent a diagnosis, or in other domains a decision, a prediction, etc. Quinlan also describes how this representation can improve decision making by "softening" absolute thresholds들.e, it provides the information necessary to avoid making absolute decisions when the specific information at hand doesn't warrant it.Multiversion inductive programmingA generalized practical software engineering technique that is emerging from these inductive AI techniques들.e., PDP networks and decision trees들s that of inductive programming in its own right (Partridge, 1997). Hitherto, these inductive techniques have been viewed as interesting AI techniques that have been demonstrated on a few unrelated and specialized problems.However, it is now becoming clear that inductive techniques can constitute a widely applicable software development technology드 further practical option for the software engineer. Many AI problems relate to very complex systems, the human body or human language are just two examples. We can collect numerous examples of specific behaviors of these complex phenomena, but we cannot specify how they function in detail듮hese are termed data-defined problems. In attempting to construct computer software to tackle such problems, diagnose diseases, say, or communicate in English, the software engineer is faced with a mass of data but precious little in the way of a detailed specification to implement. The problem-specific information offers little hope of a classical, specifi-cation-based, route to software construction, but invites an inductive approach.Inductive approaches, by there very nature, are problematic: Who is to say which (of very many) generalizations of the data is correct? Is the available data properly representative of the problem? Is the data skewed, corrupted, noisy? But there are also some advantages associated with inductive approaches: software construction is an automatic, algorithmi-cally determined procedure, once the data has been analyzed and organized. When the initial data processing has been completed, the cost of software development is cheap because machine time for automatic induction is much cheaper than person time for manual algorithm development. The automatic process is also much more controllable, potentially engineerable듮he initial conditions determine the outcome.  Page 197These factors make multiversion software engineering with inductive technologies much more economically viable than it is with classical, manual programming technologies. Multiple versions (based on, say, different data selections) can be routinely constructed and used collectively to yield highly reliable software systems. By voting or averaging over the results of a diverse version set, high performance software systems can be built from not very reliable components.Consider three different versions of a piece of software each of which is only 67\% reliable when tested. These would not seem to be promising components from which to build a good multiversion system. But suppose that they did not make the same errors, none of the same errors들.e., on 100 tests, version 1 failed on the first 33, version 2 on the second 33, and version 3 on the third 33듮hey all get the 100th test correct. Then a majority vote decision strategy with these three (individually poor) versions would yield a 100\% correct system.The majority-vote performance of the three-version system is better than any of the individual versions because the versions are diverse. With respect to voting (as opposed to, say, averaging across all three individual results), the necessary diversity is lack of coincident failure들.e, no two versions failure on the same input드nd lack of identical failure들.e. no two versions fail with the same wrong answer (see Partridge and Krzanowski, 1997, for full details and definitions).Suffice it to say that version diversity is the key characteristic to be engineered into multiversion software in order to reap substantial benefits from the extra costs of constructing multiple versions. But both the exact nature of the required diversity, and how to systematically engineer it, are non-simple problems. The general strategy is to construct the versions independently using different methodologies. This should give versions that fail independently, which would be close to maximum coincident-failure diversity, and it exploits the belief that diverse processes should lead to diverse products.The severe cost constraints on the development of multiple programmed versions has curtailed experimentation, but the relative cheapness of inductively generated versions as well as the algorithmic nature of the generation process opens up the possibility of a multiverison approach to all data-defined problems. It should be noted that such problems are by no means the exclusive preserve of AI. Earlier discussions (in Chapters 2 and 3) should have made it clear that most practical software engineering problems are, in reality, more or less data defined.The application of a multiversion inductive programming approach to software engineering is in its infancy (see Partridge and Yates, 1996,  Page 198for an example). But inductive approaches, such as PDP, have now demonstrated their worth in a wide range of practical applications (Widrow, Rumelhart and Lehr, 1994, provide a long and diverse list). Add to this the potential benefits of a well-engineered multiversion enhancement (at little extra cost because multiple versions are always developed anyway), and a further blossoming of these AI technologies in practical software engineering is to be expected. Figure 7.5 illustrates a draft methodology to support such an engineering discipline of multiver-sion inductive software engineering. Figure 7.5 "A methodological framework for multiversion inductive software engineering" (from Partridge, 1997)  Page 199In summary, this brief excursion into the AI subworld of machine learning has, I hope, left you with some food for thought, if not with concrete ideas of how to engineer self-adaptive software. Your thoughts should now extend to the notion of self-adaptive software systems as a route to practical Al software, but with considerable reservation (given the current state of the art in ML). Nevertheless, a need for self-adaptive software systems is not to be equated with the full horror of unconstrained self-modifying code. At a minimum it may amount to no more than onetime adaptation to a set of data that is taken to define a problem들.e. inductive generation of versions. We must proceed with caution, but proceed in this direction we must. The current technology in ML is not such as to occasion great expectations, but neither is it such as to engender great pessimism. Some ML mechanisms are now just on the edge of practical viability, and there is every reason to expect considerable progress in the next few years.In addition to the possibility of increasing software power through the use of ML, there is the potential use of ML techniques in a new approach to software development methodology: build a basic system and then train it to the required level. PDP systems demonstrate one means of achieving this goal, and inductive generalization techniques provide us with another.  Page 201CHAPTER 8Expert Systems' The Success StoryFar too much has been written about expert systems already, and in this brief chapter I have no intention of rehashing the time-worn examples of MYCIN, TEIRESIAS, R1, etc. going through their repertoire of tricks. The purpose of this chapter is to satisfy the obligation to provide some survey of this Al subfield in a book that is about the engineering of AI-software systems, for expert systems do have a strong claim to be the major area of practical application of AI tools and techniques. But this claim is not accepted by all who are in a position to evaluate it critically. This subfield, like most AI-related phenomena, is not devoid of contention and disputation, far from it.My goal is not to go into the arguments for and against the various claims for expert systems technology; it is simply to look at, and see what is to be learned from, the short but intense history of trying to build expert systems듫ractical AI-software (in some cases at least). Provided that we steer clear of the more grandiose systems (and the more mundane ones) there is, I believe, a growing body of practical Al-system building know-how being amassed within the general area of expert systems.Expert systems as Al softwarePart of the argument that surrounds this Al subfield hinges on the claims for the AI nature of the software systems constructed. At this distance into this book, the less than totally inattentive reader will appreciate the fruit-  Page 202lessness of attempts to definitionally circumscribe the subclass of expert systems that are A1 software. We shall do better to begin to characterize the extremes (of AI-ishness in expert systems), and then accept that the large intervening grey area is full of specific examples that could be argued for and against endlessly when we have nothing better to do.It is clear that some expert systems are examples of practical AI software, and equally clearly some are not. At the non-Al end of this spectrum, we find knowledge bases that are known to contain all of the relevant information (with respect to the specific problem that the proposed expert system tackles), that are operated on with a straightforward mechanism of logical inference, and that generate results (i.e. decisions, predictions, etc.) which are decidably correct or incorrect on the basis of an abstract decision procedure. As any of these three constraints are relaxed, so we begin to move in the direction of AI-ish expert systems. These non-Al expert systems are conventional software engineering problems implemented by means of AI techniques, which may, or may not, be the most effective implementation strategy.At the clearly-Al end of the spectrum, we find that it is necessary to use knowledge bases which are (and will always be) incomplete, that mechanisms for reasoning with uncertainty or probabilistic inference are needed to operate effectively on these knowledge bases, and that system results are not always classifiable as simply correct or incorrect: we have to come to grips with some notion of adequacy. Another, almost complementary, way to view this point is that AI systems must use heuristics: some rules will be heuristic and mechanisms of reasoning with uncertainty will involve heuristics. And heuristics, by their very nature, are not guaranteed always to give good results. They sometimes lead to poor results and even to errors듭uch, sadly, is the nature of heuristics.One further point to bear in mind is the 'level' at which the original problem is tackled듪r the scope of the problem to be addressed as it is termed below. For example, at the non-Al end of our spectrum we can aim to implement a system that simply plays legal chess; at the other extreme we can aim to construct the next world chess champion. They are both chess-playing programs, but there the similarity ends.Between these two extremes of expert systems there are all combinations of possibilities드 continuum of AI-ishness in expert systems, if you will. Luckily we do have to try to untangle the mess, but I hope that the foregoing will enable you to understand how such widely differing opinions, all from informed persons, can arise in this general area.  Page 203Engineering expert systemsSo some expert systems are definitely AI software, and some lesser portion of these are robust and reliable Al software (we must beware of what is perhaps a small proportion of a small proportion, but we are still clearly in the realms of the finite, as opposed to the infinitesimal, I believe). How were they built?Although not exactly hot off the press, Hayes-Roth, Waterman and Lenat (1983) still make a good attempt to answer this question. They take it for granted that the development of an expert system is an evolutionary, incremental process, and because of the nature of classical expert systems' technology the process boils down to one of the extraction and representation of a human expert's knowledge. Figure 8.1 illustrates their five stages in the evolution of an expert system. Figure 8.1 "Stages in the evolution of an expert system" from Hayes-Roth, Waterman and Lenat, 1983, Table 1.3 p. 24  Page 204A critical part of the first stage, Identification, is determining the scope of the problem to be addressed. This is probably a general characteristic of all attempts to implement AI- software and was referred to above in our discussion of the AI-ishness of expert systems. My unsubtle example of two sorts of chess-playing program might lead you to think that this is a fairly trivial point; it is not. In fact, deciding on the exact scope of the problem to be tackled can be a crucial determinant of whether the project will blossom and flourish, or wither and die. Most AI-type problems at the level that many humans can deal with effectively are way beyond the current state of the art for practical software engineering. So most problems need to be trimmed down drastically. But if you trim them too much, you may have a tractable problem that is, however, lacking all semblance of AI-ishness. And what's more disastrous, it may be resistant to subsequent attempts to broaden its scope to include some AI features. You may recall our earlier discussions of building AI muscles on a conventional software engineering skeleton, for this sort of strategy to succeed the skeleton must have appropriate places upon which the muscles can be developed. We must design for change, but for change in ways that are quite alien to conventional software engineering. We must address the basic problem of evolutionary software development: we must ensure that "the user's operational system will be flexible enough to accommodate unplanned evolution paths" (Boehm, 1988, p.63).In the Conceptualization stage, the task is to identify and explicate the key concepts, relations and information flow patterns in the particular perspective on the problem identified in the previous stage. This is the stage where the problem is carved up, in an informal way, into the major modules, subtasks, and control strategies for inter-task interactions. In this stage the intertangled web of the real-world problem is broken into independent parts, and then decisions have to be made as to how to satisfactorily approximate the original problem by adding well-defined control strategies to allow the selected modules to communicate with each other.This is a critical stage for the subsequent evolutionary development of the implemented system: certain modularization schemes will inhibit subsequent development of the heuristics that will be needed to add AI to the system, while others, in which the heuristic in question operates totally within a module, will not.The Formalization stage is self-explanatory, but we should note that formalization usually has a side-effect of introducing further constraints in our conceptualization of the problem. The cautions given with respect to the Conceptualization stage also apply here.  Page 205In terms of current expert systems' technology, the Implementation stage amounts to formulating the rules that constitute the knowledge base, and the control strategy to be used for reasoning over this knowledge base. After this we are in a position to produce an executable prototype system.The last stage, which Hayes-Roth, Waterman and Lenat call Testing, concerns evaluating the performance of the prototype and introducing modifications as indicated by the domain expert's critique of the system.All of the foregoing might lead you to believe that the development of an expert system is a linear process rather like the development of conventional software with just the names of the stages changed in order to pretend that something new is happening. This is quite wrong, but also quite understandable, for Figure 8.1 gives us only part of the picture, as it were. Figure 8.2 indicates that the stages do not form a simple linear progression.As you can now see (in Figure 8.2) there are a number of control paths back from each stage to an earlier stage들.e, there are loops in the methodology which suddenly makes it reminiscent of the RUDE cycle and iterative development schemes in general. But, if you want to be awkward, you might insist that this sort of diagram is not really different from the old waterfall model of conventional software development (see Figure 4.6).The main point of difference (apart from the stages involved) is the weight and 'meaning' associated with the arrows that take us back to an earlier stage in the development process. In the waterfall model there is meant to be steady linear progress from left to right, and only an occasional need to nip back to an earlier stage to correct an oversight or minor error. In fact, the use made of the 'back' arrows can be equated with the lack of time, attention and expertise of the system developer. Whereas in Figure 8.2, the back arrows are expected to carry more traffic and are not simply a result of a poor attempt at system development (although that will, of course, add to the degree to which the back arrows are used). In expert systems' development it is accepted at the outset that a smooth linear progression from initial idea to adequate system is not possible. The nature of the problems and of the target systems is such that iteration (i.e. use of the back arrows) is a crucial feature of the development methodology itself.  Page 206Figure 8.2 ''Control flow in expert systems' development" from Hayes-Roth, Waterman and Lenat, 1983, Figure 5.5, p. 139  Page 207One further and important point to make about the development of expert systems, which the above illustrations do not bring out, is that they are never 'finished'. The cynical software engineer might want to say much the same thing about conventional software systems듮hey too are often never finished. But this putative similarity is itself very similar in nature to the previous one about back arrows in the development process들.e, there is an 'echo' of the AI-system phenomenon in conventional software system development, but in practice there is a qualitative difference between these two manifestations of superficially similar phenomena. In conventional software engineering there is (or ought to be if things are done properly) a comprehensive, well-defined specification of the desired system, and in principle at least the software is finished when it meets this specification. In expert systems there is no such detailed and comprehensive specification of the desired software, and not because of any inadequacy on the part of the expert-system developers but because of the nature of the problem들t is typically a performance-based specification (recall this general feature of AI problems from Chapter 2). In this particular AI domain, the behavior of human experts is the main source of information as to what we might expect to accomplish with an expert system, and this, of course, provides a problem 'specification' that is a far cry from the sort of specification that we can and do generate for say, a conventional inventory management system.The more specific reason why an expert system is never finished is two-fold:1. The knowledge base is never complete. There is thus always the possibility of adding more knowledge to improve performance or broaden the scope of the system.2. There is often no absolutely correct answer for the system to produce. There is thus always the possibility of adding (or improving existing) heuristics in order to achieve better results득etter diagnoses, better predictions, etc.Clearly, these points of difference mesh with the characteristics of AI-ish expert systems that I gave earlier. As we move away from the AI-end of the spectrum these characteristics diminish in importance or disappear altogether, and similarly so will these points of difference.  Page 208The lessons of expert systems for engineering Al softwareThe first lesson to be learned from our whistle-stop tour of expert systems technology is that it constitutes an existence proof of the possibility of engineering AI software. Some would dispute this claim, but I think that there are definitely some (perhaps quite a small number) of AI-ish expert systems out and about in the real world and doing a useful job. The number of such systems and the level of AI that we find in them is not such as to warrant great jubilation but it is a beginning: a small step for expert systems but a big step for AI-systems engineering듮o coin a phrase.An important cache of examples of practical Al software is to be found at the Digital Equipment Corporation (DEC). For well over ten years, DEC has been collaborating with researchers at Carnegie Mellon University with the express aim of developing expert systems to tackle various problems associated with the selling of sophisticated computer systems. The fruits of this collaborative effort have the somewhat cryptic names of R1, XSEL, XCON, etc. The R1 system, for example, checks customer orders of computer systems. It is designed to minimize the shipping out of wrong or incompatible components. This example is of particular interest for a number of reasons: firstly, expert system technology was explicitly employed to address an outstanding problem that a conventional software engineering approach failed to solve; secondly, the benefits of the R1 system were easily quantifiable (at least in terms of dollars saved as a result of less incorrect orders shipped out); and thirdly, the systems has now been successfully used for many years and a lot of empirical data has been collected and published (e.g. frequency of occurrence of errors, ease of maintainability). This last point is particularly important because so very few expert systems have been subjected to close study during prolonged usage, and we know that maintenance (i.e. post-delivery activities) is an extremely important element of practical software technology. It is one thing to build a piece of software that passes all acceptance tests for immediate use; it is quite another for that software to be amenable to subsequent modification (either to remove the residual errors as and when they surface, or to enhance its functionality in response to changing needs). DEC's expert systems seem to satisfy both sorts of requirement. Figure 8.3 gives us some data from the R1 expert system collected over a period of three years. It illustrates the contention that basic software problems (such as the number of different kinds of situations that R1 did not deal effectively with듢raphed as "total distinct problems") are no worse for expert systems than they are for conventional software. This discovery is not one that is greeted with great jubilation  Page 209but it is, nevertheless, quite a comfort within a technology as new and untried, and potentially unstable, as that of expert systems.Figure 8.3 A sketch of R1's performance (from Bachant and McDermott, 1984, p. 31)A general introduction to this family of expert systems and the full results of empirical study of the R1 system can be found in McDermott (1981) and in McDermott and Bachant (1984). And a long-term study of XSEL is provided by Mumford and MacDonald (1989).  Page 210A second lesson, with more immediate impact but still quite general, is that the development of AI software cannot be squeezed into the conventional mould. We must come to grips with the notions of partial specifications whose adequacy is explored by means of working prototype systems. And we must similarly learn to do without the comfort of thinking that system behavior is provably correct even in principle. The relatively recent flurry of expert system building activity and its failure to result in a similar flurry of practical expert systems should teach us that the problems posed by AI software are not to be taken lightly. They are deep and difficult problems, and we must recognize this fact.A further twist to the conventional wisdom that experience with expert systems has exemplified is the importance of the HOW in building AI software. It is important that the system's reasoning is (or can be presented as if it is) analogous to human reasoning in similar situations. Expert systems' technology, based as it is on a knowledge base of rules and a mechanism of logical inference, shows us the importance of this point. Self-explanation capabilities are made much of in expert systems' technology (too much of, but that's beside the current point). Many expert systems have the ability to 'explain' their line of reasoning that led to a particular outcome. This 'explanation' of how a specific conclusion was generated, which is a simple chaining of IF...THEN...IF...THEN... from the rules that the inference mechanism employed, is designed to be easy for the human expert to understand.As an example, a car engine diagnosis system when given a set of symptoms (such as 'engine won't start' and 'turns over slowly') might, en route to a diagnosis, ask the question 'are the lights dim?' The user is then at liberty to ask the expert system 'why are you asking this question?', and the system's explanation will come from the particular chain of inferences that it is building. It might respond:It has been established that1. the car won't start, and2. the engine turns over slowlyTherefore, if2. the engine turns over slowly, and3. the lights are dimthenthere is evidence that the problem is4. the battery is flat [Rule 123].  Page 211Of course, the reasoning is likely to be much more complex that this, but the point is that a fairly good explanation can be automatically generated from a rule-based and logical-inference-driven system. In the above example, the clauses numbered 2,3 and 4 constitute one of the car-engine diagnostic rules in the role base. If we can suspend disbelief for one moment and assume that the explanation is not transparently obvious, the user might want to further clarify this explanation. Further probing can typically be done using 'how' queries. For example, 'how did you know that the engine turns over slowly?' In this case the system should respond that 'the engine turns over slowly was a given fact', but it might have been an inference from another role in which case the system would have retrieved and displayed the relevant rule. This could in turn suggest more 'how' queries and so the system-user explanatory dialogue can continue.The real point is not whether he or she actually uses such rule-chain-ing reasoning but that he or she can readily appreciate the validity (or not) of such an 'explanation' of specific system behavior. Now, why is this so important? You may recall Michie and Johnston's (1984) argument, (presented in Chapter 4) which elevates this benefit of rule-based systems to become the supreme goal of Al들.e. the goal of making complex technology more readily understandable. And while I'm setting up links back to earlier topics, recall also the decompiling problem and how it must be tackled (a Chapter 6 topic).The importance of this HOW constraint on AI-software function stems from the nature of the basic problems. Iterative system development requires that someone can understand the specific system behavior in terms of general system structure듮he closer the human developer can relate to the way the computer system is operating, the easier it is to achieve the necessary understanding. So the HOW constraint is necessary for system development. It is also necessary for both validation and use of an adequate system. Validation of A1 software is problematic, as we discussed in Chapter 4. A long-term process of building confidence in the behavior of the system seems to be an essential component of AI-soft-ware validation, and a major feature of this confidence-building process is for the human experts to 'see' how the behavior is generated. In addition, an A1 system is expected to produce surprising results, but this class of results is composed of two different categories which must be clearly separated: there are surprising results because of intelligent reasoning that the human overlooked, and there are surprising results from problems within the system듩ot just outright errors but possibly a generally good heuristic having a bad moment (as they necessarily will from time to time). Again an understanding of the the system's reasoning  Page 212behind the surprising result will provide the basis for the necessary discrimination.A third lesson from experience with expert systems is that part (perhaps a large part) of the secret of success is choosing the right sort of problem, and the fact that most expert systems projects do not get beyond the prototype stage tells us that it is all too easy to make the wrong choice. So, do we know what the characteristics of an appropriate problem are? We know that configuration of complex computer systems was an appropriate choice, and we know that medical diagnosis wasn't. But do we know why, in general, this is so? A number of people have tried to provide the general characterization that we now seek. Luger and Stubblefield (1989) present the following criteria for selecting a problem for expert system development:1. The need for the solution justifies the cost and effort of building an expert system.2. Human expertise is not available in all situations where it is needed.3. The problem does not require either perceptual or motor skills들.e. it is a purely cognitive problem.4. The problem is relatively self-contained, in particular, that high quality solutions do not require common-sense reasoning.5. The problem cannot be solved as a conventional software engineering problem.6. Co.-operative and articulate experts exist in the problem domain.7. The problem can be limited in scope and still be usefully solvable들t can be fitted within the current scope of the technology, say, a few hundred rules per module (knowledge-base partition).A fourth lesson to be learnt is that the so-called knowledge acquisition problem is far from solved. The codification of the information upon which human expertise is widely believed to be founded들.e, the knowledge acquisition problem듣as proved to be surprisingly resistant to the straightforward strategy of inducing the expert to simply articulate his or her rules and heuristics. As a consequence, we might observe that within the expert systems community there are substantial breakaway groups who are employing alternative tactics to solve this problem. Automatically inducing a general decision tree from a set of tutorial examples is one such alternative approach that we've seen in the previous  Page 213chapter. Another group of alternatives is based upon traditional psychological techniques of collecting large amounts of relatively simple data from the experts (e.g. judgements from the experts of relatedness of pairs of specific concepts), and applying statistics to yield an appropriate representation of the knowledge (see Cooke, 1990, for a representative example). There are also radically different approaches which deny the existence of rule-based knowledge (e.g. Dreyfus and Dreyfus, 1986) and thus champion a fundamentally different solution strategies (e.g. PDP implementations, such as the "harmony theory" model of electronic circuit expertise constructed by Smolensky, 1986).In sum, the very existence of practical expert systems should boost our confidence that the many (very many) problems to be faced by the developer of practical Al software are not insurmountable. But at the same time, the ratio of expert systems started to expert systems as delivered products is sufficiently large (perhaps 100:1) to warn us against over-confidence. The few successes do not, I believe, encapsulate the silver bullet that AI-software developers might hope to discover. Success with expert systems technology must be predicated upon right choice of problem, a supportive exploratory programming environment (i.e. the right basic tools and continual behavioral feedback from the experts), and no small measure of sheer good fortune, I suspect. And what we can gain from experience with this technology (both successes and failures) is not a neat new methodology for Al-software development, but only a knowledge of where much of the conventional wisdom on software development is deficient and where future emphasis needs to be placed든.g, in evolutionary prototyping, in in-use validation.A last, but very important point, which conveniently leads us into the next chapter, is that the leap in software development complexity that the move from conventional to expert system development entails has given considerable impetus to efforts to produce software development tools. We now see a plethora of expert system building tools and shells on the market. One of the most important decisions for the novice expert system builder is which, if any, tool or shells to purchase. Having the right tool (or shell) for the job can make the difference between having a working prototype to explore at an early stage, and becoming bogged down in knowledge-base and inference-engine implementation details so that time, patience and money runs out before any working system is forthcoming.  Page 214The general point here, and the one that introduces the next chapter, is that the complexity of Al-software engineering demands significant and substantial software tools as a prerequisite듮he demand is, in fact, greater than this: we need comprehensive software support environments.  Page 215CHAPTER 9AI into Practical SoftwareAt this late stage in the book there are a number of things that should be crystal clear. To begin with, engineering practical AI software is a very difficult task, and we are only just beginning to sort out the individual problems that need to be solved. Part of the difficulty here stems from the apparent similarities between conventional software engineering and AI-software engineering. There are indeed some real and useful similarities, but there are also some deep differences masquerading as similarities. The presence of these differences does not mean that we should abandon the challenge, nor that we must 're-model' the AI systems to fit the conventional mould; recognition of these differences should lead to the development of new techniques and to radical changes of emphasis in order to devise an appropriate methodology for the engineering of artificially intelligent software.The exact nature and extent of the differences is still a subject for debate, but what is indisputable (I think) is that the move to AI software entails a leap in complexity for the software developer. And with many tasks within conventional software development straining the limits of manageable complexity, how can we hope to cope with a non-trivial further escalation of the problem? The short answer is, we can't, but what we can do is to reduce the effective complexity for the human system developer. How do we do this? We get the computer to manage much of the complexity for us, and this is done by means of a programming support environment within which we provide a comprehensive set of power tools for system designers and developers (to steal a phrase from Sheil, 1983).  Page 216Support environmentsThe notion of providing a computational environment to assist the programmer in the programming task is not new. Significantly, it was a major feature of AI programming in the days before conventional programmers fully realized its importance. And this is quite understandable once we accept the fundamental difference between what is called ''programming" in this two areas. In conventional programming we are translating a formal specification into an alternative (formal) notation that will result in an effectively computable algorithm. This translation is by no means trivial, nor is it devoid of the need for insight and imagination, but it is, at bottom, a transformation between formal notations in which some equivalence is to be preserved.Programming, from the conventional computer science viewpoint, is the transformation of a formal specification into a computational procedure for correctly realizing the given specification.Dijkstra (1989), for example, might well support this statement about programming, although I suspect he might object that it is too weak. For Dijkstra "the programmer's... main task is to give a formal proof that the program he proposes meets the equally formal functional specification" p. 1404. Programming in AI has never been this process듭ome would say, "that's; its big mistake", but I don't think so. AI programming has always been a process of exploring prototypes and evolutionary system development.Programming in AI is the exploration of behaviorally adequate approximations to an incompletely specified (and probably computationally intractable) problem.Thus "programming" in its most limited sense is just one stage in the overall process (and not a major one)듮he implementation or coding stage. Clearly, I am using the term in the looser sense of system design and development as well as actual coding. But as you can see from the two statements given above, the meaning of the word "programming" is quite context-dependent, and not subtly so. The practicing software engineer is likely to view programming as an activity that falls somewhere between my two extreme statements, but interestingly the person at the code face may well find himself somewhat closer to the AI characterization than to the computer science one. Several of the invited commentaries to Dijkstra's (1989) article took exception to what they saw as his  Page 217extreme view. Karp (1989), for example, quoted with approval the following statement:I do not view the process of programming, especially programming in the large, as that of discovering an algorithm to a pre-specified and unalterable logic. It is more often a process of discovery and adaptation, as various relevant components of a problem are examined in turn, and a goal, initially specified incompletely, is reached. (attributed to Richard Fateman, p. 1411)First, notice that a crucial word in this domain, "programming", can mean quite different things to different people (in Chapter 1 I have previously drawn attention to the problems posed by these multiple-meaning words). Bahrami (1988), in his book entitled Designing AI-Based Software, distinguishes between "AI-based programming" and "traditional'' or "algorithmic programming". But he offers us no more than a separation between "program (control)" and "knowledge" as characteristic of AI-based programming. This is, in fact, no more than a crude characterization of expert systems' technology, which is (as we know from the previous chapter)just one facet of AI.Second, notice that once again the AI viewpoint seems to be closer to that of conventional software engineering than to what I have labeled the computer science viewpoint (earlier, in Chapter 2, I distinguished model computer science problems to first make this point). And finally, if we accept the Al viewpoint, rather than the computer science one, the function of a programming language expands considerably. New demands emerge든.g, the need to support reasoning back from observation of behavior to structural cause. As a result, the management-of-complexity demand goes up as we move from conventional to Al programming. Hence support environments were needed by Al programmers almost at the outset. The emphasis in formal computer science has been more on the avoidance of complexity by, for example, erecting a "firewall" between whether the specification meets the user's need and whether an algorithm correctly implements the specification (Dijkstra, 1989, p. 1414). The approach to the complexity problem is different; it remains to be seen how much (and what sort of) complexity can and cannot be avoided.Indirect evidence of this difference of approach can be found in the nature of the favored programming languages. Thus LISP is a terrible programming language when considered in isolation and as a vehicle for conventional programming. It can't hold a candle to, say, Pascal as a  Page 218computable notation for easing the task of designing a correct and efficient implementation of a given specification and minimizing the number and severity of the inevitable bugs. But the LISP language embedded in a powerful support environment, such as INTERLISP-D, is a totally different beast. Programming in the formal computer science sense puts a premium on a succinct formal notation that can deliver correctness when employed from within a comprehensive framework of fixed and frozen constraints. Features such as strong typing have been developed in response to such pressures. AI programming raises flexibility, as demanded by experimentation and evolutionary development, as a major desideratum of the programming medium. The resultant, differently prioritized collections of requirements have led to rather different sorts of programming languages being preferred within each domain. And although some of difference in programming language preferences can be accounted for by historical accident and simple prejudice, it does seem to be difficult to reconcile these two sets of desiderata in one and the same programming language (but see type-flow analysis in the section on reverse engineering in Chapter 6). Effective and efficient programming in Al has always demanded the support of a programming environment, and not just the availability of a programming language, so it is not at all surprising that many important features of the environment notion have arisen in the AI domain.Reduction of effective complexityWith the task of programming not restricted to the problems of deriving a correct algorithm from a fixed and well-defined specification, and the lack of built-in constraints in typical AI languages, AI programming becomes an unmanageably complex process. The programmer, or system developer if you prefer, has to manage machine-executable approximations designed to exhibit some ill-defined, ideal (but probably unattainable) patterns of behavior든.g, the way native speakers recognize and generate language, or the sorts of diagnoses that an expert diagnostician generates.The programming environments of the first generation were designed to support just the task of actually writing and debugging code. Their role was primarily one of increasing the efficiency of the code- development task. But what we now require of a support environment is something much more than this: we need complete life-cycle support from requirements analysis through to system maintenance, and rather than just  Page 219increasing efficiency we need substantial help just to make the overall task doable at all. On the bright side, we should notice that, although we demand extensive and diverse assistance from the desired support environment, it is not an all-or-nothing requirement. Any useful funtionality that we can add to a system-development environment may prove to be a positive step down the long road to our final goal. Even better than this, the achievement of such small steps should make further steps easier, for it is by experience with a useful working system that we gain practical insight into what is necessary to get a more useful working system.My insistence on complete life-cycle support refers to the need to extend the role of the computer to help both before programming begins and after the system achieves adequacy. Prior to the actual programming we have the system requirements to be organized and documented, then the specification, and most importantly the design sequence to be recorded (recall our discussion of maintaining an on-line history of design knowledge of a system, Chapter 6). When an adequate system has been delivered and is in use there will be system maintenance듮raditionally considered as part of the unavoidable pain of using computer software, but not much to do with the system development process. Hence we hear much of system development environments, but precious little on system maintenance environments, and even less on life-cycle support environments that cover both ends of the process under one roof (as it were). But AI software promises to be at least as hard as conventional software to maintain, and probably much harder when we have to deal with the extra complexities of machine-learning mechanisms.The maintenance task is, of course, just basic system development within the RUDE cycle, and thus involves all of the same problems (with probably the extra one that the original developer is not the maintainer). Just as design history and on-line documentation are expected to ease the 'understanding', 'debug' and 'edit' stages of the development cycle, they can also be expected to aid the system maintainer. Hence the benefits of a complete life-cycle environment: all useful design and development information (or knowledge if you prefer) can be made available to the system maintainer. The Eiffel system (Meyer, 1989, and described in outline later in this chapter) provides us with an example of the beginnings of automatically generated, on-line documentation in a commercial product. This documentation tool was briefly described in Chapter 6 immediately following the ADA example.Similarly, the change of emphasis in the goals of the necessary environment듡rom that of merely speeding-up system development to one of making the unmanageable manageable들s due to the extra complexities  Page 220of AI-software development. And while we might expect the desired environments to continue in their role of adding efficiency to the processes, the raison d'etre for life-cycle environments is to make the construction of AI software possible.There are a number of quite straightforward ways that support environments can be (and have been) developed to reduce the effective complexity of system development. The effectiveness of these environments is based on the computer's fast, vast and accurate information storage and retrieval abilities. A classic paper by Winograd (1975) was one of the first to outline what we might expect from a fairly sophisticated support environment. He labeled the role that he envisaged these environments playing as moderately stupid assistance.Moderately stupid assistanceWinograd argued for the necessity of computerized assistance to offset the complexity of the software development process. Even in those early days his vision was such that it is still seen as appropriate. He outlines features that we are still striving to realize today. "The key to future programming'', he wrote, "lies in systems which understand what they are doing." We are, as I'm sure you realize, still very far from achieving this goal, but we'll return to this grand goal in a later section of this chapter.He outlined four features of the 'assistant' he envisaged:1. Semantic, as well as syntactic, error checking could be a feature of such a system, thus eliminating many errors before the program is ever run. Static errors and even implications for the dynamic semantics could be brought to the programmer's attention.2. Answering questions such as, "Is variable X global to any other subprogram?", will be based upon a supersecretary (i.e. fast, accurate storage of and retrieval from vast amounts of cross-referenced information) function. Moving to more ambitious functionality: questions such as "Are there any likely problems arising from the modification proposed?" Good answers to this type of question will require that our environment contains considerable intelligence about programming at least.3. The system should be capable of filling in trivia without bothering the programmer with the actual details that it automatically generates. The programmer might decide to expand a set, S, by two elements. It should not be necessary for the programmer to clutter his  Page 221or her thinking with the implementation details of set S. "Expand set S by two elements", is the sort of command that the programmer should be able to issue, and let the system take care of the minutiae required to implement this action.4. The system should also posses some expertise in debugging programs, nothing really grand like the tracking down and analysis of errors, but the ability to apply debugging strategies (such as tracing and backtracking) as a result of high-level human directives. The programmer might ask, "Where was the value of Z last changed?", and the system should be able to rerun and trace (i.e. do whatever's necessary, or say why it can't) an execution sequence in order to answer the question.As you can see from the date of the original reference these suggestions are not new들n fact, they're ancient by Al standards득ut many are still largely future events. Why is this? It is not because of disagreement with Winograd's vision, although there is opposition in some quarters (see, for example, Scherlis, 1989, who "summarize[s] roughly" Dijkstra's, 1989, views on teaching computing science: "software engineering as a worthy cause is eyewash, and the very notion of a 'programming tool' is corrupt" p. 1406). And the lack of a concrete realization is not because no one has tried to implement these sorts of ideas: it is because an efficient and effective implementation of these modest-seeming proposals turns out to be exceedingly difficult. Any departure from a straightforward storage and retrieval capability seems to demand the introduction of AI techniques듣euristics, plausible inference, etc. And, of course, it is because support for Al programming is essential that we need such environments in the first place. We are in a Catch-22 situation: it takes AI to make AI.But the picture is not actually this dark: more than a glimmer of light is provided by the observation that we can start small and from anywhere. We don't have to solve all of AI's problems before we can hope to solve all of AI's problems. But you can now see that Winograd's phrase "moderately stupid" entails a significant amount of intelligence in any system worthy of the epithet. It did (in 1975) and still does capture a range of environment-support characteristics which would constitute a real advance, and yet it straddles the frontiers of the state of the art: some of this functionality is now commonplace and some is still the subject of research, but with a firm promise of concrete successes to further ameliorate the programmer's lot in the near future.A second justification for this level of assistance (if one is not enough for you) comes from the nature of assistance. Assistance can be entirely  Page 222passive (help is only provided as and when it is asked for), or it can have an active element which I have taken as part of the essence of 'assistance'. It is not too inaccurate (certainly no more than the norm in this book) to say that the degree of active help and of intelligence in an assistant go hand in hand. It is not at all clear how much active 'help' the programmer will want. The answer, of course, depends on many factors including the programmer, the task at hand, the quality of the assistance, etc. Winograd opted for the conservative view, a minimum of active help, an assistant that will do just what is asked, no more no less, i.e. a moderately stupid assistant.Ramamoorthy, Shekhar and Garg (1987) provide an update on Software Development Support for AI Programs. They make the point that "existing AI development environments support just the implementation phase, which results in less reliable and hard-to-maintain programs" (p. 130-31). "An ideal software development environment should provide a powerful language for the application, as well as appropriate methodology and tools for program development" (p. 31)들n a phrase, an intelligent life-cycle support environment. They classify development environments as illustrated in Figure 9.1. Figure 9.1"Classification of development environments" from Ramamoorthy et al., 1987, Figure 4, p. 34  Page 223As is apparent from Figure 9.1, the authors see life-cycle support environments as a future event, although they do list three "efforts in this direction": Genesis (their own work, see Ramamoorthy et al., 1985), Gandalf (Haberman et al., 1985), and SAGA (Kirlis et al., 1985). But for the frontiers of the current state of the art they look towards expert system building shells, such as Intellicorp's KEE system. These sorts of systems constitute their category called "knowledge-based tools". But there are others which are not wedded to expert systems only, but which also tend to be less robust and therefore less usable in practice at the present time.A major example here is the work of Rich and Waters on the "Programmer's Apprentice" system. KBEmacs, their knowledge-based editor, provides "plans" and "cliches" to assist the system developer. They characterize ''the assistant approach" as illustrated in Figure 9.2. Figure 9.2 "A programming assistant" from Waters, Figure 2 p. 353 in Rich and Waters, 1986"The assistant interacts with the tools in the environment... in the same way as the programmer and is capable of helping the programmer do what needs to be done" (Waters, 1986, p. 352). A "cliche" is "a standard method for dealing with a task" (p. 353). They are implemented as generic parameterized procedures, so all the system developer need do is to invoke a cliche and fill it with right parameter values in order to obtain  Page 224a substantial chunk of program. A "plan" is an abstract representation of a program. The "plan formalism" is designed to represent two basic types of information: "the structure of particular programs and knowledge about cliches" (p. 354).Rich (1986) elaborates the nature of plans and presents details of the formal approach that has been used. Figure 9.3 is his plan for a search loop.Rich describes a search loop in English thus: "A search loop is a loop that has two exits in which a given predicate (the same each time) is applied to a succession of objects until either the predicate is satisfied (in which case that object is made available for use outside the loop), or the objects to be searched are exhausted" (p. 492). He observes that this type of program construct can be programmed in many different forms which can be structurally very different. Within the plan calculus the essential nature of this loop can be specified (as illustrated in Figure 9.3), and this plan will accommodate most (if not all) of the variants which are only distinguished by details that are often arbitrary and conceptually unimportant. The goal is, then, for structures, such as those illustrated in Figure 9.3, to contribute to a library of standard forms. System development will then become more a task of selecting an appropriate standard form and particularizing it with the important specific details of the problem at hand, and the 'assistant' will take care of the necessary "filling in of trivia" (as Winograd suggested it should).These plans, and thus the bones of the Programmer's Assistant, rest on formal foundations, a situational calculus in point of fact. My reason for drawing your attention to this aspect of the Programmer's Assistant is not to lead in to an elaboration of all the gruesome details of the formalism but to expose an underlying trend in the nature of support environment work-the progression from eclectic ad hoc-ery to formally founded design. The initial emergence of support environments was largely a result of evolutionary growth듣andy functions were 'tied' or just lumped together as it became apparent that a particular function could serve a useful purpose in the system development process. When there is a critical mass of such handy functions, we have a support environment. This may then be viewed as an object in its own right (and not just as a fortuitous collection of handy functions) which leads on to a more conscious effort to assess its strengths and weaknesses with respect to the general goal of effective support for the programming task. The next stage (which is also the current one) is to take one step back from specific environments, grasp the essentials of what an environment is to  Page 225achieve, and then search for a well-defined basis upon which the details of the sought-after functionality can be realized.Figure 9.3 "Plan for a search loop" from Rich, 1986, Figure 2.1, p. 493  Page 226An example of this type of development, and one that addresses a fundamental problem for system developers working within an evolutionary development paradigm, is described and demonstrated by Trenouth (1990a and 1990b). He provides a formally based framework for managing (and thus relieving the human of the necessity to manage) a population of different versions of the 'same' system. Why would you be silly enough to attempt to juggle (conceptually, of course) a variety of different versions of the same system? The answer is that, although the effects of some modifications will be clearly undesirable (and thus the modified version can be discarded), many modifications will exhibit both strong and weak points according to individual circumstances. It turns out to be necessary to explore different aspects of(or different alternatives to) system development for some period of time without having to commit ourselves irrevocably to any one particular choice.Each new version of the system has a parent (the version that was modified in order to obtain the new version), and may be the parent of any number of descendents. The basic arrangement of versions to be managed is that of a tree. Each node in the tree is a version of the system under development. The version tree is defined by intervals of existence that are attached to each software object (see below). Figure 9.4 is a schematic abstraction of such a tree of versions. Figure 9.4 "A tree of versions" adapted from Trenouth, 1990b  Page 227Figure 9.4 is abstracted from a version tree that emerged in the development of a system to implement the solution to a classic AI search-space problem. The development sequence proceeds (in a general sense) from left to right, and, as you can see, various searching strategies were tried. Only eight versions (or states of the system) are illustrated, many more were actually generated. In this illustration we can see that from the initial version, labeled S0, the system developer first tried a depth-first search strategy which resulted in a working system, S1. But the search space of the particular problem addressed was infinite along certain paths, so this version failed to terminate on occasion. The user then experimented with alternative search strategies as the figure illustrates. By modifying version S1, in different ways, the versions S2, S3 and S4 were obtained. From each of these alternatives a series of modifications (not illustrated) eventually led to three working versions that solved the initial problem with different search strategies. An important point that is not brought out in the figure is that these three alternative development paths could have been generated in almost any order (the "almost" refers simply to the fact that within a given path there is a left-to-right ordering in time).The formal basis selected for the management of this 'space' of versions is an event calculus. It is a new method of supporting software version control that uses event-annotations on objects in a project knowledge base. The events are used to generate intervals of existence for the software objects (e.g. 